This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: stoix/systems/spo/ff_spo.py, stoix/systems/spo/spo_types.py, stoix/systems/mpo/mpo_types.py, stoix/systems/mpo/discrete_loss.py, stoix/systems/mpo/continuous_loss.py, stoix/systems/search/evaluator.py, stoix/systems/search/search_types.py, stoix/base_types.py, stoix/utils/loss.py, stoix/utils/multistep.py, stoix/utils/jax_utils.py, stoix/utils/training.py, stoix/utils/running_statistics.py, stoix/networks/base.py, stoix/networks/torso.py, stoix/networks/heads.py, stoix/networks/inputs.py, stoix/networks/distributions.py, stoix/networks/utils.py, stoix/configs/system/spo/ff_spo.yaml, stoix/configs/arch/anakin.yaml, stoix/configs/network/mlp.yaml
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
stoix/
  configs/
    arch/
      anakin.yaml
    network/
      mlp.yaml
    system/
      spo/
        ff_spo.yaml
  networks/
    base.py
    distributions.py
    heads.py
    inputs.py
    torso.py
    utils.py
  systems/
    mpo/
      continuous_loss.py
      discrete_loss.py
      mpo_types.py
    search/
      evaluator.py
      search_types.py
    spo/
      ff_spo.py
      spo_types.py
  utils/
    jax_utils.py
    loss.py
    multistep.py
    running_statistics.py
    training.py
  base_types.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="stoix/systems/mpo/continuous_loss.py">
from typing import Tuple, Union

import chex
import jax
import jax.numpy as jnp
from tensorflow_probability.substrates.jax.distributions import (
    Distribution,
    Independent,
    MultivariateNormalDiag,
    Normal,
)

from stoix.networks.distributions import AffineTanhTransformedDistribution
from stoix.systems.mpo.mpo_types import DualParams

# These functions were initially taken and modified from Acme's MPO implementation:

_MPO_FLOAT_EPSILON = 1e-8
_MIN_LOG_TEMPERATURE = -18.0
_MIN_LOG_ALPHA = -18.0

Shape = Tuple[int]
DType = type(jnp.float32)


def compute_weights_and_temperature_loss(
    q_values: chex.Array,
    epsilon: float,
    temperature: chex.Array,
) -> Tuple[chex.Array, chex.Array]:
    """Computes normalized importance weights for the policy optimization.

    Args:
      q_values: Q-values associated with the actions sampled from the target
        policy; expected shape [N, B].
      epsilon: Desired constraint on the KL between the target and non-parametric
        policies.
      temperature: Scalar used to temper the Q-values before computing normalized
        importance weights from them. This is really the Lagrange dual variable in
        the constrained optimization problem, the solution of which is the
        non-parametric policy targeted by the policy loss.

    Returns:
      Normalized importance weights, used for policy optimization.
      Temperature loss, used to adapt the temperature.
    """

    # Temper the given Q-values using the current temperature.
    tempered_q_values = jax.lax.stop_gradient(q_values) / temperature

    # Compute the normalized importance weights used to compute expectations with
    # respect to the non-parametric policy.
    normalized_weights = jax.nn.softmax(tempered_q_values, axis=0)
    normalized_weights = jax.lax.stop_gradient(normalized_weights)

    # Compute the temperature loss (dual of the E-step optimization problem).
    q_logsumexp = jax.scipy.special.logsumexp(tempered_q_values, axis=0)
    log_num_actions = jnp.log(q_values.shape[0] / 1.0)
    loss_temperature = epsilon + jnp.mean(q_logsumexp) - log_num_actions
    loss_temperature = temperature * loss_temperature

    return normalized_weights, loss_temperature


def compute_nonparametric_kl_from_normalized_weights(
    normalized_weights: chex.Array,
) -> chex.Array:
    """Estimate the actualized KL between the non-parametric and target policies."""

    # Compute integrand.
    num_action_samples = normalized_weights.shape[0] / 1.0
    integrand = jnp.log(num_action_samples * normalized_weights + 1e-8)

    # Return the expectation with respect to the non-parametric policy.
    return jnp.sum(normalized_weights * integrand, axis=0)


def compute_cross_entropy_loss(
    sampled_actions: chex.Array,
    normalized_weights: chex.Array,
    online_action_distribution: Distribution,
) -> chex.Array:
    """Compute cross-entropy online and the reweighted target policy.

    Args:
      sampled_actions: samples used in the Monte Carlo integration in the policy
        loss. Expected shape is [N, B, ...], where N is the number of sampled
        actions and B is the number of sampled states.
      normalized_weights: target policy multiplied by the exponentiated Q values
        and normalized; expected shape is [N, B].
      online_action_distribution: policy to be optimized.

    Returns:
      loss_policy_gradient: the cross-entropy loss that, when differentiated,
        produces the policy gradient.
    """

    # Compute the M-step loss.
    log_prob = online_action_distribution.log_prob(sampled_actions)

    # Compute the weighted average log-prob using the normalized weights.
    loss_policy_gradient = -jnp.sum(log_prob * normalized_weights, axis=0)

    # Return the mean loss over the batch of states.
    return jnp.mean(loss_policy_gradient, axis=0)


def compute_parametric_kl_penalty_and_dual_loss(
    kl: chex.Array,
    alpha: chex.Array,
    epsilon: float,
) -> Tuple[chex.Array, chex.Array]:
    """Computes the KL cost to be added to the Lagragian and its dual loss.

    The KL cost is simply the alpha-weighted KL divergence and it is added as a
    regularizer to the policy loss. The dual variable alpha itself has a loss that
    can be minimized to adapt the strength of the regularizer to keep the KL
    between consecutive updates at the desired target value of epsilon.

    Args:
      kl: KL divergence between the target and online policies.
      alpha: Lagrange multipliers (dual variables) for the KL constraints.
      epsilon: Desired value for the KL.

    Returns:
      loss_kl: alpha-weighted KL regularization to be added to the policy loss.
      loss_alpha: The Lagrange dual loss minimized to adapt alpha.
    """

    # Compute the mean KL over the batch.
    mean_kl = jnp.mean(kl, axis=0)

    # Compute the regularization.
    loss_kl = jnp.sum(jax.lax.stop_gradient(alpha) * mean_kl)

    # Compute the dual loss.
    loss_alpha = jnp.sum(alpha * (epsilon - jax.lax.stop_gradient(mean_kl)))

    return loss_kl, loss_alpha


def clip_dual_params(params: DualParams) -> DualParams:
    clipped_params = DualParams(
        log_temperature=jnp.maximum(_MIN_LOG_TEMPERATURE, params.log_temperature),
        log_alpha_mean=jnp.maximum(_MIN_LOG_ALPHA, params.log_alpha_mean),
        log_alpha_stddev=jnp.maximum(_MIN_LOG_ALPHA, params.log_alpha_stddev),
    )

    return clipped_params


def mpo_loss(
    dual_params: DualParams,
    online_action_distribution: Union[MultivariateNormalDiag, Independent],
    target_action_distribution: Union[MultivariateNormalDiag, Independent],
    target_sampled_actions: chex.Array,  # Shape [N, B, D].
    target_sampled_q_values: chex.Array,  # Shape [N, B].
    epsilon: float,
    epsilon_mean: float,
    epsilon_stddev: float,
    per_dim_constraining: bool,
    action_minimum: float,
    action_maximum: float,
) -> Tuple[chex.Array, chex.ArrayTree]:
    """Computes the decoupled MPO loss.

    Args:
        dual_params: parameters tracking the temperature and the dual variables.
        online_action_distribution: online distribution returned by the online
        policy network; expects batch_dims of [B] and event_dims of [D].
        target_action_distribution: target distribution returned by the target
        policy network; expects same shapes as online distribution.
        target_sampled_actions: actions sampled from the target policy; expects shape [N, B, D].
        target_sampled_q_values: Q-values associated with each action; expects shape [N, B].
        epsilon: KL constraint on the non-parametric auxiliary policy, the one associated with the
            dual variable called temperature.
        epsilon_mean: KL constraint on the mean of the Gaussian policy, the one associated with the
            dual variable called alpha_mean.
        epsilon_stddev: KL constraint on the stddev of the Gaussian policy, the one associated with
            the dual variable called alpha_mean.
        per_dim_constraining: whether to enforce the KL constraint on each dimension independently;
            this is the default. Otherwise the overall KL is constrained, which allows some
            dimensions to change more at the expense of others staying put.
        action_minimum: minimum action value.
        action_maximum: maximum action value.

    Returns:
        Loss, combining the policy loss, KL penalty, and dual losses required to
        adapt the dual variables.
        Stats, for diagnostics and tracking performance.
    """

    if not isinstance(target_action_distribution, Independent):
        raise ValueError("Target action distribution must be a Independent distribution.")
    if not isinstance(online_action_distribution, Independent):
        raise ValueError("Online action distribution must be Independent distribution.")

    if not isinstance(target_action_distribution.distribution, AffineTanhTransformedDistribution):
        raise ValueError("Target action distribution must be AffineTanhTransformedDistribution.")
    if not isinstance(online_action_distribution.distribution, AffineTanhTransformedDistribution):
        raise ValueError("Online action distribution must be AffineTanhTransformedDistribution.")

    # Transform dual variables from log-space.
    # Note: using softplus instead of exponential for numerical stability.
    temperature = jax.nn.softplus(dual_params.log_temperature).squeeze() + _MPO_FLOAT_EPSILON
    alpha_mean = jax.nn.softplus(dual_params.log_alpha_mean).squeeze() + _MPO_FLOAT_EPSILON
    alpha_stddev = jax.nn.softplus(dual_params.log_alpha_stddev).squeeze() + _MPO_FLOAT_EPSILON

    # Get online and target means and stddevs in preparation for decomposition.
    # We get the non bijected means and stddevs here, as we need them for the
    # decomposition.
    online_mean = online_action_distribution.distribution.distribution.mean()
    online_scale = online_action_distribution.distribution.distribution.stddev()
    target_mean = target_action_distribution.distribution.distribution.mean()
    target_scale = target_action_distribution.distribution.distribution.stddev()

    batch_size = online_mean.shape[0]
    action_dim = online_mean.shape[-1]

    # Compute normalized importance weights, used to compute expectations with
    # respect to the non-parametric policy; and the temperature loss, used to
    # adapt the tempering of Q-values.
    normalized_weights, loss_temperature = compute_weights_and_temperature_loss(
        target_sampled_q_values, epsilon, temperature
    )

    # Only needed for diagnostics: Compute estimated actualized KL between the
    # non-parametric and current target policies.
    kl_nonparametric = compute_nonparametric_kl_from_normalized_weights(normalized_weights)

    # Decompose the online policy into fixed-mean & fixed-stddev distributions.
    # This has been documented as having better performance in bandit settings,
    # see e.g. https://arxiv.org/pdf/1812.02256.pdf.
    fixed_stddev_distribution = Independent(
        AffineTanhTransformedDistribution(
            Normal(loc=online_mean, scale=target_scale), action_minimum, action_maximum
        ),
        reinterpreted_batch_ndims=1,
    )
    fixed_mean_distribution = Independent(
        AffineTanhTransformedDistribution(
            Normal(loc=target_mean, scale=online_scale), action_minimum, action_maximum
        ),
        reinterpreted_batch_ndims=1,
    )

    # Compute the decomposed policy losses.
    loss_policy_mean = compute_cross_entropy_loss(
        target_sampled_actions, normalized_weights, fixed_stddev_distribution
    )
    loss_policy_stddev = compute_cross_entropy_loss(
        target_sampled_actions, normalized_weights, fixed_mean_distribution
    )

    # Compute the decomposed KL between the target and online policies.
    if per_dim_constraining:
        kl_mean = target_action_distribution.distribution.kl_divergence(
            fixed_stddev_distribution.distribution
        )  # Shape [B, D].
        kl_stddev = target_action_distribution.distribution.kl_divergence(
            fixed_mean_distribution.distribution
        )  # Shape [B, D].

        chex.assert_shape(kl_mean, (batch_size, action_dim))
        chex.assert_shape(kl_stddev, (batch_size, action_dim))
    else:
        kl_mean = target_action_distribution.kl_divergence(fixed_stddev_distribution)  # Shape [B].
        kl_stddev = target_action_distribution.kl_divergence(fixed_mean_distribution)  # Shape [B].
        chex.assert_shape(kl_mean, (batch_size,))
        chex.assert_shape(kl_stddev, (batch_size,))

    # Compute the alpha-weighted KL-penalty and dual losses to adapt the alphas.
    loss_kl_mean, loss_alpha_mean = compute_parametric_kl_penalty_and_dual_loss(
        kl_mean, alpha_mean, epsilon_mean
    )
    loss_kl_stddev, loss_alpha_stddev = compute_parametric_kl_penalty_and_dual_loss(
        kl_stddev, alpha_stddev, epsilon_stddev
    )

    # Combine losses.
    loss_policy = loss_policy_mean + loss_policy_stddev
    loss_kl_penalty = loss_kl_mean + loss_kl_stddev
    loss_dual = loss_alpha_mean + loss_alpha_stddev + loss_temperature
    loss = loss_policy + loss_kl_penalty + loss_dual

    # Create statistics.
    loss_info = {
        "temperature": temperature,
        "alpha_mean": alpha_mean,
        "alpha_stddev": alpha_stddev,
        "loss_temperature": loss_temperature,
        "loss_alpha_mean": loss_alpha_mean,
        "loss_alpha_stddev": loss_alpha_stddev,
        "loss_policy_mean": loss_policy_mean,
        "loss_policy_stddev": loss_policy_stddev,
        "loss_kl_mean": loss_kl_mean,
        "loss_kl_stddev": loss_kl_stddev,
        "kl_mean": kl_mean,
        "kl_stddev": kl_stddev,
        "kl_nonparametric": kl_nonparametric,
    }

    return loss, loss_info
</file>

<file path="stoix/systems/mpo/discrete_loss.py">
from typing import Tuple

import chex
import jax
import jax.numpy as jnp
from tensorflow_probability.substrates.jax.distributions import Categorical

from stoix.systems.mpo.mpo_types import CategoricalDualParams

# These functions are largely taken from Acme's MPO implementation:

_MPO_FLOAT_EPSILON = 1e-8
_MIN_LOG_TEMPERATURE = -18.0
_MIN_LOG_ALPHA = -18.0

Shape = Tuple[int]
DType = type(jnp.float32)


def categorical_mpo_loss(
    dual_params: CategoricalDualParams,
    online_action_distribution: Categorical,
    target_action_distribution: Categorical,
    q_values: chex.Array,  # Shape [D, B].
    epsilon: float,
    epsilon_policy: float,
) -> Tuple[chex.Array, chex.ArrayTree]:
    """Computes the MPO loss for a categorical policy.

    Args:
        dual_params: parameters tracking the temperature and the dual variables.
        online_action_distribution: online distribution returned by the online
            policy network; expects batch_dims of [B] and event_dims of [D].
        target_action_distribution: target distribution returned by the target
            policy network; expects same shapes as online distribution.
        q_values: Q-values associated with every action; expects shape [D, B].
        epsilon: KL constraint on the non-parametric auxiliary policy, the one
            associated with the dual variable called temperature.
        epsilon_policy: KL constraint on the categorical policy, the one
            associated with the dual variable called alpha.


    Returns:
      Loss, combining the policy loss, KL penalty, and dual losses required to
        adapt the dual variables.
      Stats, for diagnostics and tracking performance.
    """

    q_values = jnp.transpose(q_values)  # [D, B] --> [B, D].

    # Transform dual variables from log-space.
    # Note: using softplus instead of exponential for numerical stability.
    temperature = get_temperature_from_params(dual_params).squeeze()
    alpha = jax.nn.softplus(dual_params.log_alpha).squeeze() + _MPO_FLOAT_EPSILON

    # Compute the E-step logits and the temperature loss, used to adapt the
    # tempering of Q-values.
    (
        logits_e_step,
        loss_temperature,
    ) = compute_weights_and_temperature_loss(  # pytype: disable=wrong-arg-types  # jax-ndarray
        q_values=q_values,
        logits=target_action_distribution.logits,
        epsilon=epsilon,
        temperature=temperature,
    )
    action_distribution_e_step = Categorical(logits=logits_e_step)

    # Only needed for diagnostics: Compute estimated actualized KL between the
    # non-parametric and current target policies.
    kl_nonparametric = action_distribution_e_step.kl_divergence(target_action_distribution)

    # Compute the policy loss.
    loss_policy = action_distribution_e_step.cross_entropy(online_action_distribution)
    loss_policy = jnp.mean(loss_policy)

    # Compute the regularization.
    kl = target_action_distribution.kl_divergence(online_action_distribution)
    mean_kl = jnp.mean(kl, axis=0)
    loss_kl = jax.lax.stop_gradient(alpha) * mean_kl

    # Compute the dual loss.
    loss_alpha = alpha * (epsilon_policy - jax.lax.stop_gradient(mean_kl))

    # Combine losses.
    loss_dual = loss_alpha + loss_temperature
    loss = loss_policy + loss_kl + loss_dual

    # Create statistics.
    loss_info = {
        "temperature": temperature,
        "alpha": alpha,
        "loss_temperature": loss_temperature.mean(),
        "loss_alpha": loss_alpha.mean(),
        "loss_policy": loss_policy.mean(),
        "loss_kl": loss_kl.mean(),
        "kl_nonparametric": kl_nonparametric.mean(),
        "entropy_online": online_action_distribution.entropy().mean(),
        "entropy_target": target_action_distribution.entropy().mean(),
        "kl_mean_rel": mean_kl / epsilon_policy,
        "kl_q_rel": jnp.mean(kl_nonparametric) / epsilon,
        "q_min": jnp.mean(jnp.min(q_values, axis=0)),
        "q_max": jnp.mean(jnp.max(q_values, axis=0)),
    }

    return loss, loss_info


def compute_weights_and_temperature_loss(
    q_values: chex.Array,
    logits: chex.Array,
    epsilon: float,
    temperature: chex.Array,
) -> Tuple[chex.Array, chex.Array]:
    """Computes normalized importance weights for the policy optimization.

    Args:
      q_values: Q-values associated with the actions sampled from the target
        policy; expected shape [B, D].
      logits: Parameters to the categorical distribution with respect to which the
        expectations are going to be computed.
      epsilon: Desired constraint on the KL between the target and non-parametric
        policies.
      temperature: Scalar used to temper the Q-values before computing normalized
        importance weights from them. This is really the Lagrange dual variable in
        the constrained optimization problem, the solution of which is the
        non-parametric policy targeted by the policy loss.

    Returns:
      Normalized importance weights, used for policy optimization.
      Temperature loss, used to adapt the temperature.
    """

    # Temper the given Q-values using the current temperature.
    tempered_q_values = jax.lax.stop_gradient(q_values) / temperature

    # Compute the E-step normalized logits.
    unnormalized_logits = tempered_q_values + jax.nn.log_softmax(logits, axis=-1)
    logits_e_step = jax.nn.log_softmax(unnormalized_logits, axis=-1)

    # Compute the temperature loss (dual of the E-step optimization problem).
    # Note that the log normalizer will be the same for all actions, so we choose
    # only the first one.
    log_normalizer = unnormalized_logits[:, 0] - logits_e_step[:, 0]
    loss_temperature = temperature * (epsilon + jnp.mean(log_normalizer))

    return logits_e_step, loss_temperature


def clip_categorical_mpo_params(params: CategoricalDualParams) -> CategoricalDualParams:
    return params._replace(
        log_temperature=jnp.maximum(_MIN_LOG_TEMPERATURE, params.log_temperature),
        log_alpha=jnp.maximum(_MIN_LOG_ALPHA, params.log_alpha),
    )


def get_temperature_from_params(params: CategoricalDualParams) -> chex.Array:
    return jax.nn.softplus(params.log_temperature) + _MPO_FLOAT_EPSILON
</file>

<file path="stoix/utils/loss.py">
from typing import Tuple

import chex
import jax
import jax.numpy as jnp
import rlax
import tensorflow_probability.substrates.jax as tfp
from tensorflow_probability.substrates.jax.distributions import Distribution

tfd = tfp.distributions

# These losses are generally taken from rlax but edited to explicitly take in a batch of data.
# This is because the original rlax losses are not batched and are meant to be used with vmap,
# which is much slower.


def ppo_clip_loss(
    pi_log_prob_t: chex.Array, b_pi_log_prob_t: chex.Array, gae_t: chex.Array, epsilon: float
) -> chex.Array:
    ratio = jnp.exp(pi_log_prob_t - b_pi_log_prob_t)
    loss_actor1 = ratio * gae_t
    loss_actor2 = (
        jnp.clip(
            ratio,
            1.0 - epsilon,
            1.0 + epsilon,
        )
        * gae_t
    )
    loss_actor = -jnp.minimum(loss_actor1, loss_actor2)
    loss_actor = loss_actor.mean()
    return loss_actor


def ppo_penalty_loss(
    pi_log_prob_t: chex.Array,
    b_pi_log_prob_t: chex.Array,
    gae_t: chex.Array,
    beta: float,
    pi: Distribution,
    b_pi: Distribution,
) -> Tuple[chex.Array, chex.Array]:
    ratio = jnp.exp(pi_log_prob_t - b_pi_log_prob_t)
    kl_div = b_pi.kl_divergence(pi).mean()
    objective = ratio * gae_t - beta * kl_div
    loss_actor = -objective.mean()
    return loss_actor, kl_div


def dpo_loss(
    pi_log_prob_t: chex.Array,
    b_pi_log_prob_t: chex.Array,
    gae_t: chex.Array,
    alpha: float,
    beta: float,
) -> chex.Array:
    log_diff = pi_log_prob_t - b_pi_log_prob_t
    ratio = jnp.exp(log_diff)
    is_pos = (gae_t >= 0.0).astype(jnp.float32)
    r1 = ratio - 1.0
    drift1 = jax.nn.relu(r1 * gae_t - alpha * jax.nn.tanh(r1 * gae_t / alpha))
    drift2 = jax.nn.relu(log_diff * gae_t - beta * jax.nn.tanh(log_diff * gae_t / beta))
    drift = drift1 * is_pos + drift2 * (1 - is_pos)
    loss_actor = -(ratio * gae_t - drift).mean()
    return loss_actor


def clipped_value_loss(
    pred_value_t: chex.Array, behavior_value_t: chex.Array, targets_t: chex.Array, epsilon: float
) -> chex.Array:
    value_pred_clipped = behavior_value_t + (pred_value_t - behavior_value_t).clip(
        -epsilon, epsilon
    )
    value_losses = jnp.square(pred_value_t - targets_t)
    value_losses_clipped = jnp.square(value_pred_clipped - targets_t)
    value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()

    return value_loss


def categorical_double_q_learning(
    q_logits_tm1: chex.Array,
    q_atoms_tm1: chex.Array,
    a_tm1: chex.Array,
    r_t: chex.Array,
    d_t: chex.Array,
    q_logits_t: chex.Array,
    q_atoms_t: chex.Array,
    q_t_selector: chex.Array,
) -> chex.Array:
    """Computes the categorical double Q-learning loss. Each input is a batch."""
    batch_indices = jnp.arange(a_tm1.shape[0])
    # Scale and shift time-t distribution atoms by discount and reward.
    target_z = r_t[:, jnp.newaxis] + d_t[:, jnp.newaxis] * q_atoms_t
    # Select logits for greedy action in state s_t and convert to distribution.
    p_target_z = jax.nn.softmax(q_logits_t[batch_indices, q_t_selector.argmax(-1)])
    # Project using the Cramer distance and maybe stop gradient flow to targets.
    target = jax.vmap(rlax.categorical_l2_project)(target_z, p_target_z, q_atoms_tm1)
    # Compute loss (i.e. temporal difference error).
    logit_qa_tm1 = q_logits_tm1[batch_indices, a_tm1]
    td_error = tfd.Categorical(probs=target).cross_entropy(tfd.Categorical(logits=logit_qa_tm1))

    return td_error


def q_learning(
    q_tm1: chex.Array,
    a_tm1: chex.Array,
    r_t: chex.Array,
    d_t: chex.Array,
    q_t: chex.Array,
    huber_loss_parameter: chex.Array,
) -> jnp.ndarray:
    """Computes the double Q-learning loss. Each input is a batch."""
    batch_indices = jnp.arange(a_tm1.shape[0])
    # Compute Q-learning n-step TD-error.
    target_tm1 = r_t + d_t * jnp.max(q_t, axis=-1)
    td_error = target_tm1 - q_tm1[batch_indices, a_tm1]
    if huber_loss_parameter > 0.0:
        batch_loss = rlax.huber_loss(td_error, huber_loss_parameter)
    else:
        batch_loss = rlax.l2_loss(td_error)

    return jnp.mean(batch_loss)


def double_q_learning(
    q_tm1: chex.Array,
    q_t_value: chex.Array,
    a_tm1: chex.Array,
    r_t: chex.Array,
    d_t: chex.Array,
    q_t_selector: chex.Array,
    huber_loss_parameter: chex.Array,
) -> jnp.ndarray:
    """Computes the double Q-learning loss. Each input is a batch."""
    batch_indices = jnp.arange(a_tm1.shape[0])
    # Compute double Q-learning n-step TD-error.
    target_tm1 = r_t + d_t * q_t_value[batch_indices, q_t_selector.argmax(-1)]
    td_error = target_tm1 - q_tm1[batch_indices, a_tm1]
    if huber_loss_parameter > 0.0:
        batch_loss = rlax.huber_loss(td_error, huber_loss_parameter)
    else:
        batch_loss = rlax.l2_loss(td_error)

    return jnp.mean(batch_loss)


def td_learning(
    v_tm1: chex.Array,
    r_t: chex.Array,
    discount_t: chex.Array,
    v_t: chex.Array,
    huber_loss_parameter: chex.Array,
) -> chex.Array:
    """Calculates the temporal difference error. Each input is a batch."""
    target_tm1 = r_t + discount_t * v_t
    td_errors = target_tm1 - v_tm1
    if huber_loss_parameter > 0.0:
        batch_loss = rlax.huber_loss(td_errors, huber_loss_parameter)
    else:
        batch_loss = rlax.l2_loss(td_errors)
    return jnp.mean(batch_loss)


def categorical_td_learning(
    v_logits_tm1: chex.Array,
    v_atoms_tm1: chex.Array,
    r_t: chex.Array,
    d_t: chex.Array,
    v_logits_t: chex.Array,
    v_atoms_t: chex.Array,
) -> chex.Array:
    """Implements TD-learning for categorical value distributions. Each input is a batch."""

    # Scale and shift time-t distribution atoms by discount and reward.
    target_z = r_t[:, jnp.newaxis] + d_t[:, jnp.newaxis] * v_atoms_t

    # Convert logits to distribution.
    v_t_probs = jax.nn.softmax(v_logits_t)

    # Project using the Cramer distance and maybe stop gradient flow to targets.
    target = jax.vmap(rlax.categorical_l2_project)(target_z, v_t_probs, v_atoms_tm1)

    td_error = tfd.Categorical(probs=target).cross_entropy(tfd.Categorical(logits=v_logits_tm1))

    return jnp.mean(td_error)


def munchausen_q_learning(
    q_tm1: chex.Array,
    q_tm1_target: chex.Array,
    a_tm1: chex.Array,
    r_t: chex.Array,
    d_t: chex.Array,
    q_t_target: chex.Array,
    entropy_temperature: chex.Array,
    munchausen_coefficient: chex.Array,
    clip_value_min: chex.Array,
    huber_loss_parameter: chex.Array,
) -> chex.Array:
    action_one_hot = jax.nn.one_hot(a_tm1, q_tm1.shape[-1])
    q_tm1_a = jnp.sum(q_tm1 * action_one_hot, axis=-1)
    # Compute double Q-learning loss.
    # Munchausen term : tau * log_pi(a|s)
    munchausen_term = entropy_temperature * jax.nn.log_softmax(
        q_tm1_target / entropy_temperature, axis=-1
    )
    munchausen_term_a = jnp.sum(action_one_hot * munchausen_term, axis=-1)
    munchausen_term_a = jnp.clip(munchausen_term_a, a_min=clip_value_min, a_max=0.0)

    # Soft Bellman operator applied to q
    next_v = entropy_temperature * jax.nn.logsumexp(q_t_target / entropy_temperature, axis=-1)
    target_q = jax.lax.stop_gradient(
        r_t + munchausen_coefficient * munchausen_term_a + d_t * next_v
    )
    td_error = target_q - q_tm1_a
    if huber_loss_parameter > 0.0:
        batch_loss = rlax.huber_loss(td_error, huber_loss_parameter)
    else:
        batch_loss = rlax.l2_loss(td_error)
    batch_loss = jnp.mean(batch_loss)
    return batch_loss


def quantile_regression_loss(
    dist_src: chex.Array,
    tau_src: chex.Array,
    dist_target: chex.Array,
    huber_param: float = 0.0,
) -> chex.Array:
    """Compute (Huber) QR loss between two discrete quantile-valued distributions.

    See "Distributional Reinforcement Learning with Quantile Regression" by
    Dabney et al. (https://arxiv.org/abs/1710.10044).

    Args:
        dist_src: source probability distribution.
        tau_src: source distribution probability thresholds.
        dist_target: target probability distribution.
        huber_param: Huber loss parameter, defaults to 0 (no Huber loss).
        stop_target_gradients: bool indicating whether or not to apply stop gradient
        to targets.

    Returns:
        Quantile regression loss.
    """

    batch_indices = jnp.arange(dist_src.shape[0])

    # Calculate quantile error.
    delta = dist_target[batch_indices, None, :] - dist_src[batch_indices, :, None]
    delta_neg = (delta < 0.0).astype(jnp.float32)
    delta_neg = jax.lax.stop_gradient(delta_neg)
    weight = jnp.abs(tau_src[batch_indices, :, None] - delta_neg)

    # Calculate Huber loss.
    if huber_param > 0.0:
        loss = rlax.huber_loss(delta, huber_param)
    else:
        loss = jnp.abs(delta)
    loss *= weight

    # Average over target-samples dimension, sum over src-samples dimension.
    return jnp.sum(jnp.mean(loss, axis=-1), axis=-1)


def quantile_q_learning(
    dist_q_tm1: chex.Array,
    tau_q_tm1: chex.Array,
    a_tm1: chex.Array,
    r_t: chex.Array,
    d_t: chex.Array,
    dist_q_t_selector: chex.Array,
    dist_q_t: chex.Array,
    huber_param: float = 0.0,
) -> chex.Array:
    """Implements Q-learning for quantile-valued Q distributions.

    See "Distributional Reinforcement Learning with Quantile Regression" by
    Dabney et al. (https://arxiv.org/abs/1710.10044).

    Args:
        dist_q_tm1: Q distribution at time t-1.
        tau_q_tm1: Q distribution probability thresholds.
        a_tm1: action index at time t-1.
        r_t: reward at time t.
        d_t: discount at time t.
        dist_q_t_selector: Q distribution at time t for selecting greedy action in
        target policy. This is separate from dist_q_t as in Double Q-Learning, but
        can be computed with the target network and a separate set of samples.
        dist_q_t: target Q distribution at time t.
        huber_param: Huber loss parameter, defaults to 0 (no Huber loss).
        stop_target_gradients: bool indicating whether or not to apply stop gradient
        to targets.

    Returns:
        Quantile regression Q learning loss.
    """
    batch_indices = jnp.arange(a_tm1.shape[0])

    # Only update the taken actions.
    dist_qa_tm1 = dist_q_tm1[batch_indices, :, a_tm1]

    # Select target action according to greedy policy w.r.t. dist_q_t_selector.
    q_t_selector = jnp.mean(dist_q_t_selector, axis=1)
    a_t = jnp.argmax(q_t_selector, axis=-1)
    dist_qa_t = dist_q_t[batch_indices, :, a_t]

    # Compute target, do not backpropagate into it.
    dist_target = r_t[:, jnp.newaxis] + d_t[:, jnp.newaxis] * dist_qa_t
    dist_target = jax.lax.stop_gradient(dist_target)

    return quantile_regression_loss(dist_qa_tm1, tau_q_tm1, dist_target, huber_param).mean()
</file>

<file path="stoix/utils/training.py">
from typing import Callable, Optional, Union

from omegaconf import DictConfig


def make_learning_rate_schedule(
    init_lr: float, num_updates: int, num_epochs: int, num_minibatches: int
) -> Callable:
    """Makes a very simple linear learning rate scheduler.

    Args:
        init_lr: initial learning rate.
        num_updates: number of updates.
        num_epochs: number of epochs.
        num_minibatches: number of minibatches.

    Note:
        We use a simple linear learning rate scheduler based on the suggestions from a blog on PPO
        implementation details which can be viewed at http://tinyurl.com/mr3chs4p
        This function can be extended to have more complex learning rate schedules by adding any
        relevant arguments to the system config and then parsing them accordingly here.
    """

    def linear_scedule(count: int) -> float:
        frac: float = 1.0 - (count // (num_epochs * num_minibatches)) / num_updates
        return init_lr * frac

    return linear_scedule


def make_learning_rate(
    init_lr: float, config: DictConfig, num_epochs: int, num_minibatches: Optional[int] = None
) -> Union[float, Callable]:
    """Returns a constant learning rate or a learning rate schedule.

    Args:
        init_lr: initial learning rate.
        config: system configuration.
        num_epochs: number of epochs.
        num_minibatches: number of minibatches.

    Returns:
        A learning rate schedule or fixed learning rate.
    """
    if num_minibatches is None:
        num_minibatches = 1

    if config.system.decay_learning_rates:
        return make_learning_rate_schedule(
            init_lr, config.arch.num_updates, num_epochs, num_minibatches
        )
    else:
        return init_lr
</file>

<file path="stoix/configs/system/spo/ff_spo.yaml">
# --- Defaults FF-SPO ---

system_name: ff_spo # Name of the system.

# --- RL hyperparameters ---
actor_lr: 3e-4 # Learning rate for actor network
critic_lr: 3e-4 # Learning rate for critic network
dual_lr: 1e-3  # the learning rate of the alpha optimizer
tau: 0.005  # smoothing coefficient for target networks
rollout_length: 32 # Number of environment steps per vectorised environment.
epochs: 128 # Number of epochs per training data batch.
warmup_steps: 0  # Number of steps to collect before training.
total_buffer_size: 65536 # Total effective size of the replay buffer across all devices and vectorised update steps. This means each device has a buffer of size buffer_size//num_devices which is further divided by the update_batch_size. This value must be divisible by num_devices*update_batch_size.
total_batch_size: 32 # Total effective number of samples to train on. This means each device has a batch size of batch_size/num_devices which is further divided by the update_batch_size. This value must be divisible by num_devices*update_batch_size.
sample_sequence_length: 32 # Number of steps to consider for each element of the batch.
period : 1 # Period of the sampled sequences.
gamma: 0.99 # Discounting factor.
gae_lambda: 0.95 # Lambda value for Generalized Advantage Estimation (GAE) computation.
vf_coef: 1.0 # Critic weight in the loss function.
max_grad_norm: 0.5 # Maximum norm of the gradients for a weight update.
decay_learning_rates: False # Whether learning rates should be linearly decayed during training.

epsilon: 0.5 # KL constraint on the non-parametric auxiliary policy, the one associated with the dual variable called temperature.
epsilon_policy : 1e-3 #  KL constraint on the categorical policy, the one associated with the dual variable called alpha.
init_log_temperature: 3. # initial value for the temperature in log-space, note a softplus (rather than an exp) will be used to transform this.
init_log_alpha: 3. # initial value for the alpha value in log-space, note a softplus (rather than an exp) will be used to transform this.

# --- Search Hyperparameters ---
num_particles: 16 # Number of particles in SPO's Sequential Monte Carlo (SMC) Search.
search_depth: 4 # The depth of the SMC search.
root_exploration_dirichlet_alpha : 1.0 # Dirichlet noise parameter for the root node sampled actions.
root_exploration_dirichlet_fraction : 0.0 # Fraction of the noise to interpolate with the prior policy.
resampling:
  mode: period # Resampling method: 'period' or 'ess' (effective sample size)
  period: 4 # If using period mode this is the frequency of resampling
  ess_threshold: 0.5 # If using ess mode this is the threshold for resampling
temperature:
  adaptive: True # Whether to use adaptive temperature for search
  fixed_temperature: 0.5 # Fixed temperature value if adaptive is False
search_gamma: 1.0 # Discount factor used for search (can differ from training gamma)
search_gae_lambda: 1.0 # GAE lambda parameter used in search
</file>

<file path="stoix/networks/distributions.py">
from typing import Any, Optional, Sequence

import chex
import distrax
import jax
import jax.numpy as jnp
import numpy as np
import tensorflow_probability.substrates.jax as tfp
from tensorflow_probability.substrates.jax.distributions import (
    Beta,
    Categorical,
    Distribution,
    TransformedDistribution,
)

tfb = tfp.bijectors


class AffineTanhTransformedDistribution(TransformedDistribution):
    """Distribution followed by tanh and then affine transformations."""

    def __init__(
        self,
        distribution: Distribution,
        minimum: float,
        maximum: float,
        epsilon: float = 1e-3,
        validate_args: bool = False,
    ) -> None:
        """Initialize the distribution with a tanh and affine bijector.

        Args:
          distribution: The distribution to transform.
          minimum: Lower bound of the target range.
          maximum: Upper bound of the target range.
          epsilon: epsilon value for numerical stability.
            epsilon is used to compute the log of the average probability distribution
            outside the clipping range, i.e. on the interval
            [-inf, atanh(inverse_affine(minimum))] for log_prob_left and
            [atanh(inverse_affine(maximum)), inf] for log_prob_right.
          validate_args: Passed to super class.
        """
        # Calculate scale and shift for the affine transformation to achieve the range
        # [minimum, maximum] after the tanh.
        scale = (maximum - minimum) / 2.0
        shift = (minimum + maximum) / 2.0

        # Chain the bijectors
        joint_bijector = tfb.Chain([tfb.Shift(shift), tfb.Scale(scale), tfb.Tanh()])

        super().__init__(
            distribution=distribution, bijector=joint_bijector, validate_args=validate_args
        )

        # Computes the log of the average probability distribution outside the
        # clipping range, i.e. on the interval [-inf, atanh(inverse_affine(minimum))] for
        # log_prob_left and [atanh(inverse_affine(maximum)), inf] for log_prob_right.
        self._min_threshold = minimum + epsilon
        self._max_threshold = maximum - epsilon
        min_inverse_threshold = self.bijector.inverse(self._min_threshold)
        max_inverse_threshold = self.bijector.inverse(self._max_threshold)
        # average(pdf) = p/epsilon
        # So log(average(pdf)) = log(p) - log(epsilon)
        log_epsilon = jnp.log(epsilon)
        # Those 2 values are differentiable w.r.t. model parameters, such that the
        # gradient is defined everywhere.
        self._log_prob_left = self.distribution.log_cdf(min_inverse_threshold) - log_epsilon
        self._log_prob_right = (
            self.distribution.log_survival_function(max_inverse_threshold) - log_epsilon
        )

    def log_prob(self, event: chex.Array) -> chex.Array:
        # Without this clip there would be NaNs in the inner tf.where and that
        # causes issues for some reasons.
        event = jnp.clip(event, self._min_threshold, self._max_threshold)
        return jnp.where(
            event <= self._min_threshold,
            self._log_prob_left,
            jnp.where(event >= self._max_threshold, self._log_prob_right, super().log_prob(event)),
        )

    def mode(self) -> chex.Array:
        return self.bijector.forward(self.distribution.mode())

    def entropy(self, seed: chex.PRNGKey = None) -> chex.Array:
        return self.distribution.entropy() + self.bijector.forward_log_det_jacobian(
            self.distribution.sample(seed=seed), event_ndims=0
        )

    @classmethod
    def _parameter_properties(cls, dtype: Optional[Any], num_classes: Any = None) -> Any:
        td_properties = super()._parameter_properties(dtype, num_classes=num_classes)
        del td_properties["bijector"]
        return td_properties


class ClippedBeta(Beta):
    """Beta distribution with clipped samples."""

    def sample(
        self,
        sample_shape: Sequence[int] = (),
        seed: Optional[chex.PRNGKey] = None,
        name: str = "sample",
        **kwargs: Any
    ) -> chex.Array:
        _epsilon = 1e-7
        # Call the original sample method
        sample = super().sample(sample_shape, seed, name, **kwargs)
        # Clip the sample to avoid being too close to 0 and 1
        # This is important for numerical stability
        clipped_sample = jnp.clip(sample, _epsilon, 1 - _epsilon)
        return clipped_sample


class DiscreteValuedTfpDistribution(Categorical):
    """This is a generalization of a categorical distribution.

    The support for the DiscreteValued distribution can be any real valued range,
    whereas the categorical distribution has support [0, n_categories - 1] or
    [1, n_categories]. This generalization allows us to take the mean of the
    distribution over its support.
    """

    def __init__(
        self,
        values: chex.Array,
        logits: Optional[chex.Array] = None,
        probs: Optional[chex.Array] = None,
        name: str = "DiscreteValuedDistribution",
    ):
        """Initialization.

        Args:
          values: Values making up support of the distribution. Should have a shape
            compatible with logits.
          logits: An N-D Tensor, N >= 1, representing the log probabilities of a set
            of Categorical distributions. The first N - 1 dimensions index into a
            batch of independent distributions and the last dimension indexes into
            the classes.
          probs: An N-D Tensor, N >= 1, representing the probabilities of a set of
            Categorical distributions. The first N - 1 dimensions index into a batch
            of independent distributions and the last dimension represents a vector
            of probabilities for each class. Only one of logits or probs should be
            passed in.
          name: Name of the distribution object.
        """
        parameters = dict(locals())
        self._values = np.asarray(values)
        self._logits: Optional[chex.Array] = None
        self._probs: Optional[chex.Array] = None

        if logits is not None:
            logits = jnp.asarray(logits)
            chex.assert_shape(logits, (..., *self._values.shape))

        if probs is not None:
            probs = jnp.asarray(probs)
            chex.assert_shape(probs, (..., *self._values.shape))

        super().__init__(logits=logits, probs=probs, name=name)

        self._parameters = parameters

    @property
    def values(self) -> chex.Array:
        return self._values

    @property
    def logits(self) -> chex.Array:
        if self._logits is None:
            self._logits = jax.nn.log_softmax(self._probs)
        return self._logits

    @property
    def probs(self) -> chex.Array:
        if self._probs is None:
            self._probs = jax.nn.softmax(self._logits)
        return self._probs

    @classmethod
    def _parameter_properties(cls, dtype: np.dtype, num_classes: Any = None) -> Any:
        return {
            "values": tfp.util.ParameterProperties(
                event_ndims=None, shape_fn=lambda shape: (num_classes,), specifies_shape=True
            ),
            "logits": tfp.util.ParameterProperties(event_ndims=1),
            "probs": tfp.util.ParameterProperties(event_ndims=1, is_preferred=False),
        }

    def _sample_n(self, key: chex.PRNGKey, n: int) -> chex.Array:
        indices = super()._sample_n(key=key, n=n)
        return jnp.take_along_axis(self._values, indices, axis=-1)

    def mean(self) -> chex.Array:
        """Overrides the Categorical mean by incorporating category values."""
        return jnp.sum(self.probs_parameter() * self._values, axis=-1)

    def variance(self) -> chex.Array:
        """Overrides the Categorical variance by incorporating category values."""
        dist_squared = jnp.square(jnp.expand_dims(self.mean(), -1) - self._values)
        return jnp.sum(self.probs_parameter() * dist_squared, axis=-1)

    def _event_shape(self) -> chex.Array:
        return jnp.zeros((), dtype=jnp.int32)

    def _event_shape_tensor(self) -> chex.Array:
        return []


class MultiDiscreteActionDistribution(distrax.Distribution):
    """A multi discrete action distribution where each discrete
        subspace can have a different number of dimensions.

    Copied from Kinetix.
    """

    def __init__(self, flat_logits: chex.Array, number_of_dims_per_distribution: list[int]) -> None:
        self.distributions = []
        total_dims = 0
        for dims in number_of_dims_per_distribution:
            self.distributions.append(
                distrax.Categorical(logits=flat_logits[..., total_dims : total_dims + dims])
            )
            total_dims += dims

    def _sample_n(self, key: chex.PRNGKey, n: int) -> Any:
        rngs = jax.random.split(key, len(self.distributions))
        samples = [
            jnp.expand_dims(d._sample_n(rng, n), axis=-1)
            for rng, d in zip(rngs, self.distributions)
        ]
        return jnp.concatenate(samples, axis=-1)

    def log_prob(self, value: Any) -> chex.Array:
        return sum(d.log_prob(value[..., i]) for i, d in enumerate(self.distributions))

    def entropy(self) -> chex.Array:
        return sum(d.entropy() for d in self.distributions)

    def event_shape(self) -> Sequence[int]:
        return ()
</file>

<file path="stoix/networks/inputs.py">
import chex
import jax
import jax.numpy as jnp
from flax import linen as nn


class ArrayInput(nn.Module):
    """JAX Array Input. Used for any input that is already a JAX array."""

    @nn.compact
    def __call__(self, embedding: chex.Array) -> chex.Array:
        return embedding


class FeatureInput(nn.Module):
    """Used for inputs that are specific attributes of some observation type."""

    feature_name: str

    @nn.compact
    def __call__(self, input_object: chex.ArrayTree) -> chex.Array:
        embedding = getattr(input_object, self.feature_name)
        return embedding


class EmbeddingActionInput(nn.Module):
    """Observation/Embedding and Action Input."""

    @nn.compact
    def __call__(self, embedding: chex.Array, action: chex.Array) -> chex.Array:
        """Concatenates observation/embedding and action."""
        x = jnp.concatenate([embedding, action], axis=-1)
        return x


class EmbeddingActionOnehotInput(nn.Module):
    """Observation/Embedding and Action One-hot Input."""

    action_dim: int

    @nn.compact
    def __call__(self, observation_embedding: chex.Array, action: chex.Array) -> chex.Array:
        action_one_hot = jax.nn.one_hot(action, self.action_dim)
        x = jnp.concatenate([observation_embedding, action_one_hot], axis=-1)
        return x
</file>

<file path="stoix/networks/utils.py">
from typing import Callable, Dict

import chex
from flax import linen as nn


def parse_activation_fn(activation_fn_name: str) -> Callable[[chex.Array], chex.Array]:
    """Get the activation function."""
    activation_fns: Dict[str, Callable[[chex.Array], chex.Array]] = {
        "relu": nn.relu,
        "tanh": nn.tanh,
        "silu": nn.silu,
        "elu": nn.elu,
        "gelu": nn.gelu,
        "sigmoid": nn.sigmoid,
        "softplus": nn.softplus,
        "swish": nn.swish,
        "identity": lambda x: x,
        "none": lambda x: x,
        "normalise": nn.standardize,
        "softmax": nn.softmax,
        "log_softmax": nn.log_softmax,
        "log_sigmoid": nn.log_sigmoid,
    }
    return activation_fns[activation_fn_name]


def parse_rnn_cell(rnn_cell_name: str) -> nn.RNNCellBase:
    """Get the rnn cell."""
    rnn_cells: Dict[str, Callable[[chex.Array], chex.Array]] = {
        "lstm": nn.LSTMCell,
        "optimised_lstm": nn.OptimizedLSTMCell,
        "gru": nn.GRUCell,
        "mgu": nn.MGUCell,
        "simple": nn.SimpleCell,
    }
    return rnn_cells[rnn_cell_name]
</file>

<file path="stoix/utils/running_statistics.py">
"""
Utility functions to compute running statistics.
Taken and modified from
Acme https://github.com/google-deepmind/acme/blob/master/acme/jax/running_statistics.py
"""

import dataclasses
import types
from typing import (
    Any,
    Callable,
    Dict,
    NamedTuple,
    Optional,
    Sequence,
    Tuple,
    Type,
    TypeVar,
    Union,
)

import chex
import jax
import jax.numpy as jnp
import numpy as np
import tree
from jax import Array

Path = Tuple[Any, ...]
"""Path in a nested structure.

  A path is a tuple of indices (normally strings for maps and integers for
  arrays and tuples) that uniquely identifies a subtree in the nested structure.
  See
  https://tree.readthedocs.io/en/latest/api.html#tree.map_structure_with_path
  for more details.
"""


def fast_map_structure(func: Callable, *structure: chex.ArrayTree) -> chex.ArrayTree:
    """Faster map_structure implementation which skips some error checking."""
    flat_structure = (tree.flatten(s) for s in structure)
    entries = zip(*flat_structure)
    # Arbitrarily choose one of the structures of the original sequence (the last)
    # to match the structure for the flattened sequence.
    return tree.unflatten_as(structure[-1], [func(*x) for x in entries])


def fast_map_structure_with_path(func: Callable, *structure: chex.ArrayTree) -> chex.ArrayTree:
    """Faster map_structure_with_path implementation."""
    head_entries_with_path = tree.flatten_with_path(structure[0])
    if len(structure) > 1:
        tail_entries = (tree.flatten(s) for s in structure[1:])
        entries_with_path = [e[0] + e[1:] for e in zip(head_entries_with_path, *tail_entries)]
    else:
        entries_with_path = head_entries_with_path
    # Arbitrarily choose one of the structures of the original sequence (the last)
    # to match the structure for the flattened sequence.
    return tree.unflatten_as(structure[-1], [func(*x) for x in entries_with_path])


def _psum_over_axes(value: Array, pmap_axis_names: Optional[Sequence[str]]) -> Array:
    """Apply psum over multiple axes sequentially."""
    if pmap_axis_names is None:
        return value

    result = value
    for axis_name in pmap_axis_names:
        result = jax.lax.psum(result, axis_name=axis_name)
    return result


def _is_prefix(a: Path, b: Path) -> bool:
    """Returns whether `a` is a prefix of `b`."""
    return b[: len(a)] == a


def _zeros_like(nest: chex.ArrayTree, dtype: Optional[jnp.dtype] = None) -> chex.ArrayTree:
    return jax.tree_util.tree_map(lambda x: jnp.zeros(x.shape, dtype or x.dtype), nest)


def _ones_like(nest: chex.ArrayTree, dtype: Optional[jnp.dtype] = None) -> chex.ArrayTree:
    return jax.tree_util.tree_map(lambda x: jnp.ones(x.shape, dtype or x.dtype), nest)


@chex.dataclass(frozen=True)
class NestedMeanStd:
    """A container for running statistics (mean, std) of possibly nested data."""

    mean: chex.ArrayTree
    std: chex.ArrayTree


@chex.dataclass(frozen=True)
class RunningStatisticsState(NestedMeanStd):
    """Full state of running statistics computation."""

    count: Union[int, Array]
    summed_variance: chex.ArrayTree


@dataclasses.dataclass(frozen=True)
class NestStatisticsConfig:
    """Specifies how to compute statistics for Nests with the same structure.

    Attributes:
      paths: A sequence of Nest paths to compute statistics for. If there is a
        collision between paths (one is a prefix of the other), the shorter path
        takes precedence.
    """

    paths: Tuple[Path, ...] = ((),)


def _is_path_included(config: NestStatisticsConfig, path: Path) -> bool:
    """Returns whether the path is included in the config."""
    # A path is included in the config if it corresponds to a tree node that
    # belongs to a subtree rooted at the node corresponding to some path in
    # the config.
    return any(_is_prefix(config_path, path) for config_path in config.paths)


def initialize_statistics(nest: chex.ArrayTree) -> RunningStatisticsState:
    """Initializes the running statistics for the given nested structure."""
    dtype: jnp.dtype = jnp.float64 if jax.config.jax_enable_x64 else jnp.float32

    return RunningStatisticsState(  # type: ignore
        count=0.0,
        mean=_zeros_like(nest, dtype=dtype),
        summed_variance=_zeros_like(nest, dtype=dtype),
        # Initialize with ones to make sure normalization works correctly
        # in the initial state.
        std=_ones_like(nest, dtype=dtype),
    )


def initialize_statistics_from_data(
    nest: chex.ArrayTree,
    data_sample: chex.ArrayTree,
    *,
    config: Optional[NestStatisticsConfig] = None,
    weights: Optional[Array] = None,
    std_min_value: float = 5e-4,
    std_max_value: float = 5e4,
    pmap_axes: Optional[Union[str, Sequence[str]]] = None,
    validate_shapes: bool = True,
) -> RunningStatisticsState:
    """Initializes the running statistics for the given nested structure from a data sample."""
    if config is None:
        config = NestStatisticsConfig()
    init_running_statistics = initialize_statistics(nest)
    return update_statistics(
        state=init_running_statistics,
        batch=data_sample,
        config=config,
        weights=weights,
        std_min_value=std_min_value,
        std_max_value=std_max_value,
        pmap_axes=pmap_axes,
        validate_shapes=validate_shapes,
    )


def _validate_batch_shapes(
    batch: chex.ArrayTree, reference_sample: chex.ArrayTree, batch_dims: Tuple[int, ...]
) -> None:
    """Verifies shapes of the batch leaves against the reference sample.

    Checks that batch dimensions are the same in all leaves in the batch.
    Checks that non-batch dimensions for all leaves in the batch are the same
    as in the reference sample.

    Arguments:
      batch: the nested batch of data to be verified.
      reference_sample: the nested array to check non-batch dimensions.
      batch_dims: a Tuple of indices of batch dimensions in the batch shape.

    Returns:
      None.
    """

    def validate_node_shape(reference_sample: Array, batch: Array) -> None:
        expected_shape: Tuple[int, ...] = batch_dims + reference_sample.shape
        assert batch.shape == expected_shape, f"{batch.shape} != {expected_shape}"

    fast_map_structure(validate_node_shape, reference_sample, batch)


def convert_pmap_axes_names(
    pmap_axes: Optional[Union[str, Sequence[str]]]
) -> Optional[Sequence[str]]:
    """Converts pmap axes names to a list of strings."""
    # Handle multiple pmap axes
    pmap_axis_names: Optional[Sequence[str]] = None
    if pmap_axes is not None:
        if isinstance(pmap_axes, str):
            pmap_axis_names = [pmap_axes]
        else:
            pmap_axis_names = list(pmap_axes)

    return pmap_axis_names


def update_statistics(
    state: RunningStatisticsState,
    batch: chex.ArrayTree,
    *,
    config: Optional[NestStatisticsConfig] = None,
    weights: Optional[Array] = None,
    std_min_value: float = 1e-6,
    std_max_value: float = 1e6,
    pmap_axes: Optional[Union[str, Sequence[str]]] = None,
    validate_shapes: bool = True,
) -> RunningStatisticsState:
    """Updates the running statistics with the given batch of data.

    Note: data batch and state elements (mean, etc.) must have the same structure.

    Note: by default will use int32 for counts and float32 for accumulated
    variance. This results in an integer overflow after 2^31 data points and
    degrading precision after 2^24 batch updates or even earlier if variance
    updates have large dynamic range.
    To improve precision, consider setting jax_enable_x64 to True, see
    https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#double-64bit-precision

    Examples:
      # Single pmap axis
      updated_state = update(state, batch, pmap_axes='device')

      # Multiple pmap axes
      updated_state = update(state, batch, pmap_axes=['device', 'batch'])

    Arguments:
      state: The running statistics before the update.
      batch: The data to be used to update the running statistics.
      config: The config that specifies which leaves of the nested structure
        should the running statistics be computed for.
      weights: Weights of the batch data. Should match the batch dimensions.
        Passing a weight of 2. should be equivalent to updating on the
        corresponding data point twice.
      std_min_value: Minimum value for the standard deviation.
      std_max_value: Maximum value for the standard deviation.
      pmap_axes: Name(s) of the pmapped axis/axes. Can be a single string or a
        sequence of strings for multiple axes (e.g., ['device', 'batch']).
      validate_shapes: If true, the shapes of all leaves of the batch will be
        validated. Enabled by default. Doesn't impact performance when jitted.

    Returns:
      Updated running statistics.
    """
    if config is None:
        config = NestStatisticsConfig()
    # We require exactly the same structure to avoid issues when flattened
    # batch and state have different order of elements.
    tree.assert_same_structure(batch, state.mean)
    batch_shape: Tuple[int, ...] = tree.flatten(batch)[0].shape
    # We assume the batch dimensions always go first.
    batch_dims: Tuple[int, ...] = batch_shape[: len(batch_shape) - tree.flatten(state.mean)[0].ndim]
    batch_axis: range = range(len(batch_dims))

    # # Handle multiple pmap axes
    pmap_axis_names: Optional[Sequence[str]] = convert_pmap_axes_names(pmap_axes)

    step_increment: Union[int, Array]
    if weights is None:
        step_increment = np.prod(batch_dims)
    else:
        step_increment = jnp.sum(weights)

    # Apply psum across all specified axes
    if pmap_axis_names is not None:
        step_increment = _psum_over_axes(step_increment, pmap_axis_names)

    count: Union[int, Array] = state.count + step_increment

    # Validation is important. If the shapes don't match exactly, but are
    # compatible, arrays will be silently broadcasted resulting in incorrect
    # statistics.
    if validate_shapes:
        if weights is not None:
            if weights.shape != batch_dims:
                raise ValueError(f"{weights.shape} != {batch_dims}")
        _validate_batch_shapes(batch, state.mean, batch_dims)

    def _compute_node_statistics(
        path: Path, mean: Array, summed_variance: Array, batch: Array
    ) -> Tuple[Array, Array]:
        assert isinstance(mean, Array), type(mean)
        assert isinstance(summed_variance, Array), type(summed_variance)
        if not _is_path_included(config, path):
            # Return unchanged.
            return mean, summed_variance
        # The mean and the sum of past variances are updated with Welford's
        # algorithm using batches (see https://stackoverflow.com/q/56402955).
        diff_to_old_mean: Array = batch - mean
        if weights is not None:
            expanded_weights: Array = jnp.reshape(
                weights, list(weights.shape) + [1] * (batch.ndim - weights.ndim)
            )
            diff_to_old_mean = diff_to_old_mean * expanded_weights
        mean_update: Array = jnp.sum(diff_to_old_mean, axis=batch_axis) / count
        mean_update = _psum_over_axes(mean_update, pmap_axis_names)
        mean = mean + mean_update

        diff_to_new_mean: Array = batch - mean
        variance_update: Array = diff_to_old_mean * diff_to_new_mean
        variance_update = jnp.sum(variance_update, axis=batch_axis)
        variance_update = _psum_over_axes(variance_update, pmap_axis_names)
        summed_variance = summed_variance + variance_update
        return mean, summed_variance

    updated_stats: Union[Tuple[Array, Array], chex.ArrayTree] = fast_map_structure_with_path(
        _compute_node_statistics, state.mean, state.summed_variance, batch
    )
    # map_structure_up_to is slow, so shortcut if we know the input is not
    # structured.
    mean: chex.ArrayTree
    summed_variance: chex.ArrayTree
    if isinstance(state.mean, Array):
        mean, summed_variance = updated_stats  # type: ignore
    else:
        # Reshape the updated stats from `nest(mean, summed_variance)` to
        # `nest(mean), nest(summed_variance)`.
        mean, summed_variance = [
            tree.map_structure_up_to(state.mean, lambda s, i=idx: s[i], updated_stats)
            for idx in range(2)
        ]

    def compute_std(path: Path, summed_variance: Array, std: Array) -> Array:
        assert isinstance(summed_variance, Array)
        if not _is_path_included(config, path):
            return std
        # Summed variance can get negative due to rounding errors.
        summed_variance = jnp.maximum(summed_variance, 0)
        variance = summed_variance / count
        variance = jnp.clip(variance, jnp.square(std_min_value), jnp.square(std_max_value))
        std = jnp.sqrt(variance)
        std = jnp.clip(std, std_min_value, std_max_value)
        return std

    std: chex.ArrayTree = fast_map_structure_with_path(compute_std, summed_variance, state.std)

    return RunningStatisticsState(
        count=count, mean=mean, summed_variance=summed_variance, std=std  # type: ignore
    )


def normalize(
    batch: chex.ArrayTree, mean_std: NestedMeanStd, max_abs_value: Optional[float] = None
) -> chex.ArrayTree:
    """Normalizes data using running statistics."""

    def normalize_leaf(data: Array, mean: Array, std: Array) -> Array:
        # Only normalize inexact types.
        if not jnp.issubdtype(data.dtype, jnp.inexact):
            return data
        data = (data - mean) / std
        if max_abs_value is not None:

            data = jnp.clip(data, -max_abs_value, +max_abs_value)
        return data

    return fast_map_structure(normalize_leaf, batch, mean_std.mean, mean_std.std)


def denormalize(batch: chex.ArrayTree, mean_std: NestedMeanStd) -> chex.ArrayTree:
    """Denormalizes values in a nested structure using the given mean/std.

    Only values of inexact types are denormalized.
    See https://numpy.org/doc/stable/_images/dtype-hierarchy.png for Numpy type
    hierarchy.

    Args:
      batch: a nested structure containing batch of data.
      mean_std: mean and standard deviation used for denormalization.

    Returns:
      Nested structure with denormalized values.
    """

    def denormalize_leaf(data: Array, mean: Array, std: Array) -> Array:
        # Only denormalize inexact types.
        if not np.issubdtype(data.dtype, np.inexact):
            return data
        return data * std + mean

    return fast_map_structure(denormalize_leaf, batch, mean_std.mean, mean_std.std)


@dataclasses.dataclass(frozen=True)
class NestClippingConfig:
    """Specifies how to clip Nests with the same structure.

    Attributes:
      path_map: A map that specifies how to clip values in Nests with the same
        structure. Keys correspond to paths in the nest. Values are maximum
        absolute values to use for clipping. If there is a collision between paths
        (one path is a prefix of the other), the behavior is undefined.
    """

    path_map: Tuple[Tuple[Path, float], ...] = ()


def get_clip_config_for_path(config: NestClippingConfig, path: Path) -> NestClippingConfig:
    """Returns the config for a subtree from the leaf defined by the path."""
    # Start with an empty config.
    path_map: list[Tuple[Path, float]] = []
    for map_path, max_abs_value in config.path_map:
        if _is_prefix(map_path, path):
            return NestClippingConfig(path_map=(((), max_abs_value),))
        if _is_prefix(path, map_path):
            path_map.append((map_path[len(path) :], max_abs_value))
    return NestClippingConfig(path_map=tuple(path_map))


def clip(batch: chex.ArrayTree, clipping_config: NestClippingConfig) -> chex.ArrayTree:
    """Clips the batch."""

    def max_abs_value_for_path(path: Path, x: Array) -> Optional[float]:
        del x  # Unused, needed by interface.
        return next(
            (
                max_abs_value
                for clipping_path, max_abs_value in clipping_config.path_map
                if _is_prefix(clipping_path, path)
            ),
            None,
        )

    max_abs_values: chex.ArrayTree = fast_map_structure_with_path(max_abs_value_for_path, batch)

    def clip_leaf(data: Array, max_abs_value: Optional[float]) -> Array:
        if max_abs_value is not None:
            data = jnp.clip(data, -max_abs_value, +max_abs_value)
        return data

    return fast_map_structure(clip_leaf, batch, max_abs_values)


# Define type variable for NamedTuple subclasses
NT = TypeVar("NT", bound=NamedTuple)


def add_field_to_state(
    base_class: Type[NT], extra_field_name: str, extra_field_type: Type[Any]
) -> Type[NamedTuple]:
    """Create a new NamedTuple class by extending an existing NamedTuple class with an additional field.

    This function creates a specialized NamedTuple class that behaves in a special way:
    1. It includes all fields from the base class plus the extra field
    2. The extra field is accessible as a normal attribute (obj.extra_field_name)
    3. When unpacking the object (a, b, c = obj), only the original fields are included
    4. The _replace method works with all fields including the extra field
    5. copy.deepcopy works correctly

    This approach is useful when you need to attach metadata or state to an existing
    NamedTuple without affecting its unpacking behavior in existing code.

    Args:
        base_class: The NamedTuple class to extend (not an instance).
        extra_field_name: The name of the additional field to add to the new class.
        extra_field_type: The type of the additional field to add to the new class.

    Returns:
        A new NamedTuple class with all fields from base_class plus the extra field.
        When unpacking, only the original fields from base_class will be included.
    """
    # Get field names and annotations from the base class
    fields = getattr(base_class, "_fields", ())
    annotations = getattr(base_class, "__annotations__", {})

    # Create a new class with all original fields plus the new one
    new_fields: Dict[str, Type[Any]] = {field: annotations.get(field, Any) for field in fields}
    new_fields[extra_field_name] = extra_field_type

    # Create the new NamedTuple class with all the fields
    class_name = f"Enhanced{base_class.__name__}"

    # Create the new class
    result_class = types.new_class(
        class_name,
        (NamedTuple,),
        {},
        lambda ns: ns.update(
            {
                "__annotations__": new_fields,
                "__doc__": f"Version of {base_class.__name__} with extra field '{extra_field_name}'.",
                "__module__": base_class.__module__,
            }
        ),
    )

    # Need to preserve the _fields for proper replacement
    all_fields = getattr(result_class, "_fields", ())

    # Override __iter__ to only include base fields when unpacking
    def custom_iter(self: Any) -> Any:
        """Iterates only through the original fields, not the extra field."""
        return iter(getattr(self, field) for field in fields)

    result_class.__iter__ = custom_iter  # type: ignore

    # Fix the _replace method to ensure it still works
    def custom_replace(self: Any, **kwargs: Any) -> Any:
        """Custom replacement that works with the modified structure.

        Allows replacing any field (original or extra) while preserving the
        custom unpacking behavior.
        """
        # Get a dictionary of all current values
        current_values: Dict[str, Any] = {}
        for field in all_fields:
            current_values[field] = getattr(self, field)

        # Update with new values
        current_values.update(kwargs)

        # Create new instance with updated values
        return type(self)(**current_values)

    result_class._replace = custom_replace  # type: ignore

    # Add __getnewargs__ to support copy.deepcopy
    def custom_getnewargs(self: Any) -> tuple:
        """Return all field values for pickling/copying, including the extra field."""
        return tuple(getattr(self, field) for field in all_fields)

    result_class.__getnewargs__ = custom_getnewargs  # type: ignore

    return result_class


def create_with_running_statistics(state: NT, running_statistics: RunningStatisticsState) -> Any:
    """Add running statistics to a state instance.

    This function takes an existing state instance and attaches running statistics
    to it, creating a new version of the state class. The state
    has special unpacking behavior - when unpacked, only the original state fields
    are included, but the running_statistics field can be accessed directly.

    Key behaviors:
    1. The running_statistics field is accessible as state.running_statistics
    2. When unpacking (a, b, c = state), only the original fields are included
    3. All NamedTuple methods like _replace work on all fields
    4. copy.deepcopy works correctly

    Args:
        state: An instance of a NamedTuple state.
        running_statistics: The RunningStatisticsState instance to add to the state.

    Returns:
        A new instance of an enhanced state class that includes the running statistics
        field but excludes it from unpacking operations.
    """
    cls_type = type(state)
    new_cls_type = add_field_to_state(cls_type, "running_statistics", type(running_statistics))
    state_dict = state._asdict()
    state_dict["running_statistics"] = running_statistics
    return new_cls_type(**state_dict)  # type: ignore
</file>

<file path="stoix/configs/network/mlp.yaml">
# ---MLP PPO Networks---
actor_network:
  pre_torso:
    _target_: stoix.networks.torso.MLPTorso
    layer_sizes: [256, 256]
    use_layer_norm: False
    activation: relu
  action_head:
    _target_: stoix.networks.heads.CategoricalHead

critic_network:
  pre_torso:
    _target_: stoix.networks.torso.MLPTorso
    layer_sizes: [256, 256]
    use_layer_norm: False
    activation: relu
  critic_head:
    _target_: stoix.networks.heads.ScalarCriticHead
</file>

<file path="stoix/systems/mpo/mpo_types.py">
from typing import Dict, Union

import chex
import optax
from flashbax.buffers.trajectory_buffer import BufferState
from flax.core.frozen_dict import FrozenDict
from stoa import TimeStep
from typing_extensions import NamedTuple

from stoix.base_types import Action, Done, OnlineAndTarget, Truncated, WrapperState


class SequenceStep(NamedTuple):
    obs: chex.ArrayTree
    action: Action
    reward: chex.Array
    done: Done
    truncated: Truncated
    log_prob: chex.Array
    info: Dict


class DualParams(NamedTuple):
    log_temperature: chex.Array
    log_alpha_mean: chex.Array
    log_alpha_stddev: chex.Array


class CategoricalDualParams(NamedTuple):
    log_temperature: chex.Array
    log_alpha: chex.Array


class MPOParams(NamedTuple):
    actor_params: OnlineAndTarget
    q_params: OnlineAndTarget
    dual_params: Union[DualParams, CategoricalDualParams]


class MPOOptStates(NamedTuple):
    actor_opt_state: optax.OptState
    q_opt_state: optax.OptState
    dual_opt_state: optax.OptState


class MPOLearnerState(NamedTuple):
    params: MPOParams
    opt_states: MPOOptStates
    buffer_state: BufferState
    key: chex.PRNGKey
    env_state: WrapperState
    timestep: TimeStep


class VMPOParams(NamedTuple):
    actor_params: OnlineAndTarget
    critic_params: FrozenDict
    dual_params: Union[DualParams, CategoricalDualParams]


class VMPOOptStates(NamedTuple):
    actor_opt_state: optax.OptState
    critic_opt_state: optax.OptState
    dual_opt_state: optax.OptState


class VMPOLearnerState(NamedTuple):
    params: VMPOParams
    opt_states: VMPOOptStates
    key: chex.PRNGKey
    env_state: WrapperState
    timestep: TimeStep
    learner_step_count: int
</file>

<file path="stoix/systems/spo/spo_types.py">
from typing import Callable, Dict, NamedTuple, Tuple, TypeAlias, Union

import chex
import optax
from flax.core.frozen_dict import FrozenDict

from stoix.base_types import (
    Action,
    Done,
    Observation,
    OnlineAndTarget,
    Truncated,
    Value,
)
from stoix.systems.mpo.mpo_types import CategoricalDualParams, DualParams

_SPO_FLOAT_EPSILON = 1e-8


class SPOParams(NamedTuple):
    actor_params: OnlineAndTarget
    critic_params: OnlineAndTarget
    dual_params: Union[CategoricalDualParams, DualParams]


class SPOOptStates(NamedTuple):
    actor_opt_state: optax.OptState
    critic_opt_state: optax.OptState
    dual_opt_state: optax.OptState


class SPOTransition(NamedTuple):
    done: Done
    truncated: Truncated
    action: Action
    sampled_actions: chex.Array
    sampled_actions_weights: chex.Array
    reward: chex.Array
    search_value: Value
    obs: chex.Array
    bootstrap_obs: chex.Array
    info: Dict
    sampled_advantages: chex.Array


class SPORootFnOutput(NamedTuple):
    particle_logits: chex.Array
    particle_actions: chex.Array
    particle_env_states: chex.ArrayTree
    particle_values: chex.Array


class SPORecurrentFnOutput(NamedTuple):
    reward: chex.Array
    discount: chex.Array
    prior_logits: chex.Array
    value: chex.Array
    next_sampled_action: chex.Array


class SPOOutput(NamedTuple):
    action: Action
    sampled_action_weights: chex.Array
    sampled_actions: chex.Array
    value: chex.Array
    sampled_advantages: chex.Array
    rollout_metrics: Dict


StateEmbedding: TypeAlias = chex.ArrayTree
SPOApply = Callable[[FrozenDict, chex.PRNGKey, SPORootFnOutput], SPOOutput]
SPORootFnApply = Callable[[FrozenDict, Observation, chex.ArrayTree, chex.PRNGKey], SPORootFnOutput]
SPORecurrentFn = Callable[
    [SPOParams, chex.PRNGKey, Action, StateEmbedding], Tuple[SPORecurrentFnOutput, StateEmbedding]
]
</file>

<file path="stoix/utils/jax_utils.py">
import time
from typing import Any

import chex
import jax
import jax.numpy as jnp
import numpy as np
from colorama import Fore, Style
from jax._src.pjit import JitWrapped


def scale_gradient(g: chex.Array, scale: float = 1) -> chex.Array:
    """Scales the gradient of `g` by `scale` but keeps the original value unchanged."""
    return g * scale + jax.lax.stop_gradient(g) * (1.0 - scale)


def count_parameters(params: chex.ArrayTree) -> int:
    """Counts the number of parameters in a parameter tree."""
    return sum(x.size for x in jax.tree_util.tree_leaves(params))


def ndim_at_least(x: chex.Array, num_dims: chex.Numeric) -> chex.Array:
    """Check if the number of dimensions of `x` is at least `num_dims`."""
    if not (isinstance(x, jax.Array) or isinstance(x, np.ndarray)):
        x = jnp.asarray(x)
    return x.ndim >= num_dims


def merge_leading_dims(x: chex.Array, num_dims: chex.Numeric) -> chex.Array:
    """Merge leading dimensions.

    Note:
        This implementation is a generic function for merging leading dimensions
        extracted from Haiku.
        For the original implementation, please refer to the following link:
        (https://github.com/deepmind/dm-haiku/blob/main/haiku/_src/basic.py#L207)
    """
    # Don't merge if there aren't dimensions to merge.
    if not ndim_at_least(x, num_dims):
        return x

    new_shape = (np.prod(x.shape[:num_dims]),) + x.shape[num_dims:]
    return x.reshape(new_shape)


@jax.jit
def unreplicate_n_dims(x: chex.ArrayTree, unreplicate_depth: int = 2) -> chex.ArrayTree:
    """Unreplicates a pytree by removing the first `unreplicate_depth` axes.

    This function takes a pytree and removes some number of axes, associated with parameter
    duplication for running multiple updates across devices and in parallel with `vmap`.
    This is typically one axis for device replication, and one for the `update batch size`.
    """
    return jax.tree_util.tree_map(lambda x: x[(0,) * unreplicate_depth], x)  # type: ignore


@jax.jit
def unreplicate_batch_dim(x: chex.ArrayTree) -> chex.ArrayTree:
    """Unreplicated just the update batch dimension.
    (The dimension that is vmapped over when acting and learning)

    In stoix's case it is always the second dimension, after the device dimension.
    We simply take element 0 as the params are identical across this dimension.
    """
    return jax.tree_util.tree_map(lambda x: x[:, 0, ...], x)  # type: ignore


def aot_compile(
    fn_to_compile: JitWrapped,
    fn_name: str,
    *args: Any,
    **kwargs: Any,
) -> Any:
    """
    Compiles a JAX function ahead-of-time and prints benchmarking information.

    This function generalizes the process of tracing, lowering, and compiling
    a JAX function, making it reusable for different functions like learners,
    evaluators, etc.

    Args:
        fn_to_compile: The jitted or pmapped JAX function to be compiled.
        fn_name: A descriptive name for the function (e.g., "Learner", "Evaluator")
                 used for printing logs.
        *args: Positional arguments to be passed to the function for tracing.
        **kwargs: Keyword arguments to be passed to the function for tracing.

    Returns:
        The compiled function artifact.
    """
    print(f"{Fore.YELLOW}Compiling {fn_name} function ahead of time...{Style.RESET_ALL}")
    start_time = time.time()

    # Use the provided args and kwargs to trace the function
    traced_fn = fn_to_compile.trace(*args, **kwargs)
    lowered_fn = traced_fn.lower()
    compiled_fn = lowered_fn.compile()

    elapsed = time.time() - start_time

    # Extract cost analysis safely
    cost_analysis = compiled_fn.cost_analysis()
    flops_estimate = cost_analysis.get("flops", 0)

    print(
        f"{Fore.GREEN}{Style.BRIGHT}{fn_name} function compiled in "
        f"{elapsed:.2f} seconds.{Style.RESET_ALL}"
    )
    if flops_estimate > 0:
        print(
            f"{Fore.GREEN}{Style.BRIGHT}{fn_name} function FLOPs: "
            f"{flops_estimate / 1e9:.3f} GFlops.{Style.RESET_ALL}"
        )

    return compiled_fn
</file>

<file path="stoix/networks/heads.py">
from typing import Optional, Sequence, Tuple, Union

import chex
import distrax
import jax
import jax.numpy as jnp
import numpy as np
import tensorflow_probability.substrates.jax as tfp
from flax import linen as nn
from flax.linen.initializers import Initializer, lecun_normal, orthogonal
from tensorflow_probability.substrates.jax.distributions import (
    Categorical,
    Deterministic,
    Independent,
    MultivariateNormalDiag,
    Normal,
    TransformedDistribution,
)

from stoix.networks.distributions import (
    AffineTanhTransformedDistribution,
    ClippedBeta,
    DiscreteValuedTfpDistribution,
    MultiDiscreteActionDistribution,
)

tfb = tfp.bijectors


class CategoricalHead(nn.Module):
    action_dim: Union[int, Sequence[int]]
    kernel_init: Initializer = orthogonal(0.01)

    @nn.compact
    def __call__(self, embedding: chex.Array) -> Categorical:
        logits = nn.Dense(np.prod(self.action_dim), kernel_init=self.kernel_init)(embedding)

        if not isinstance(self.action_dim, int):
            logits = logits.reshape(self.action_dim)

        return Categorical(logits=logits)


class NormalAffineTanhDistributionHead(nn.Module):

    action_dim: int
    minimum: float
    maximum: float
    min_scale: float = 1e-3
    kernel_init: Initializer = orthogonal(0.01)

    @nn.compact
    def __call__(self, embedding: chex.Array) -> Independent:

        loc = nn.Dense(self.action_dim, kernel_init=self.kernel_init)(embedding)
        scale = (
            jax.nn.softplus(nn.Dense(self.action_dim, kernel_init=self.kernel_init)(embedding))
            + self.min_scale
        )
        distribution = Normal(loc=loc, scale=scale)

        return Independent(
            AffineTanhTransformedDistribution(distribution, self.minimum, self.maximum),
            reinterpreted_batch_ndims=1,
        )


class BetaDistributionHead(nn.Module):

    action_dim: int
    minimum: float
    maximum: float
    kernel_init: Initializer = orthogonal(0.01)

    @nn.compact
    def __call__(self, embedding: chex.Array) -> Independent:

        # Use alpha and beta >= 1 according to [Chou et. al, 2017]
        alpha = (
            jax.nn.softplus(nn.Dense(self.action_dim, kernel_init=self.kernel_init)(embedding)) + 1
        )
        beta = (
            jax.nn.softplus(nn.Dense(self.action_dim, kernel_init=self.kernel_init)(embedding)) + 1
        )
        # Calculate scale and shift for the affine transformation to achieve the range
        # [minimum, maximum].
        scale = self.maximum - self.minimum
        shift = self.minimum
        affine_bijector = tfb.Chain([tfb.Shift(shift), tfb.Scale(scale)])

        transformed_distribution = TransformedDistribution(
            ClippedBeta(alpha, beta), bijector=affine_bijector
        )

        return Independent(
            transformed_distribution,
            reinterpreted_batch_ndims=1,
        )


class MultivariateNormalDiagHead(nn.Module):

    action_dim: int
    init_scale: float = 0.3
    min_scale: float = 1e-3
    kernel_init: Initializer = orthogonal(0.01)

    @nn.compact
    def __call__(self, embedding: chex.Array) -> distrax.DistributionLike:
        loc = nn.Dense(self.action_dim, kernel_init=self.kernel_init)(embedding)
        scale = jax.nn.softplus(nn.Dense(self.action_dim, kernel_init=self.kernel_init)(embedding))
        scale *= self.init_scale / jax.nn.softplus(0.0)
        scale += self.min_scale
        return MultivariateNormalDiag(loc=loc, scale_diag=scale)


class DeterministicHead(nn.Module):
    action_dim: int
    kernel_init: Initializer = orthogonal(0.01)

    @nn.compact
    def __call__(self, embedding: chex.Array) -> chex.Array:

        x = nn.Dense(self.action_dim, kernel_init=self.kernel_init)(embedding)

        return Deterministic(x)


class ScalarCriticHead(nn.Module):
    kernel_init: Initializer = orthogonal(1.0)

    @nn.compact
    def __call__(self, embedding: chex.Array) -> chex.Array:
        return nn.Dense(1, kernel_init=self.kernel_init)(embedding).squeeze(axis=-1)


class CategoricalCriticHead(nn.Module):

    num_atoms: int = 601
    vmax: Optional[float] = None
    vmin: Optional[float] = None
    kernel_init: Initializer = orthogonal(1.0)

    @nn.compact
    def __call__(self, embedding: chex.Array) -> distrax.DistributionLike:
        vmax = self.vmax if self.vmax is not None else 0.5 * (self.num_atoms - 1)
        vmin = self.vmin if self.vmin is not None else -1.0 * vmax

        output = DiscreteValuedTfpHead(
            vmin=vmin,
            vmax=vmax,
            logits_shape=(),
            num_atoms=self.num_atoms,
            kernel_init=self.kernel_init,
        )(embedding)

        return output


class DiscreteValuedTfpHead(nn.Module):
    """Represents a parameterized discrete valued distribution.

    The returned distribution is essentially a `tfd.Categorical` that knows its
    support and thus can compute the mean value.
    If vmin and vmax have shape S, this will store the category values as a
    Tensor of shape (S*, num_atoms).

    Args:
        vmin: Minimum of the value range
        vmax: Maximum of the value range
        num_atoms: The atom values associated with each bin.
        logits_shape: The shape of the logits, excluding batch and num_atoms
        dimensions.
        kernel_init: The initializer for the dense layer.
    """

    vmin: float
    vmax: float
    num_atoms: int
    logits_shape: Optional[Sequence[int]] = None
    kernel_init: Initializer = lecun_normal()

    def setup(self) -> None:
        self._values = np.linspace(self.vmin, self.vmax, num=self.num_atoms, axis=-1)
        if not self.logits_shape:
            logits_shape: Sequence[int] = ()
        else:
            logits_shape = self.logits_shape
        self._logits_shape = (
            *logits_shape,
            self.num_atoms,
        )
        self._logits_size = np.prod(self._logits_shape)
        self._net = nn.Dense(self._logits_size, kernel_init=self.kernel_init)

    def __call__(self, inputs: chex.Array) -> distrax.DistributionLike:
        logits = self._net(inputs)
        logits = logits.reshape(logits.shape[:-1] + self._logits_shape)
        return DiscreteValuedTfpDistribution(values=self._values, logits=logits)


class DiscreteQNetworkHead(nn.Module):
    action_dim: int
    epsilon: float = 0.1
    kernel_init: Initializer = orthogonal(1.0)

    @nn.compact
    def __call__(
        self, embedding: chex.Array, epsilon: Optional[float] = None
    ) -> distrax.EpsilonGreedy:

        q_values = nn.Dense(self.action_dim, kernel_init=self.kernel_init)(embedding)

        if epsilon is None:
            epsilon = self.epsilon

        return distrax.EpsilonGreedy(preferences=q_values, epsilon=epsilon)


class PolicyValueHead(nn.Module):
    action_head: nn.Module
    critic_head: nn.Module

    @nn.compact
    def __call__(
        self, embedding: chex.Array
    ) -> Tuple[distrax.DistributionLike, Union[chex.Array, distrax.DistributionLike]]:

        action_distribution = self.action_head(embedding)
        value = self.critic_head(embedding)

        return action_distribution, value


class DistributionalDiscreteQNetwork(nn.Module):
    action_dim: int
    epsilon: float
    num_atoms: int
    vmin: float
    vmax: float
    kernel_init: Initializer = lecun_normal()

    @nn.compact
    def __call__(
        self, embedding: chex.Array
    ) -> Tuple[distrax.EpsilonGreedy, chex.Array, chex.Array]:
        atoms = jnp.linspace(self.vmin, self.vmax, self.num_atoms)
        q_logits = nn.Dense(self.action_dim * self.num_atoms, kernel_init=self.kernel_init)(
            embedding
        )
        q_logits = jnp.reshape(q_logits, (-1, self.action_dim, self.num_atoms))
        q_dist = jax.nn.softmax(q_logits)
        q_values = jnp.sum(q_dist * atoms, axis=2)
        q_values = jax.lax.stop_gradient(q_values)
        atoms = jnp.broadcast_to(atoms, (q_values.shape[0], self.num_atoms))
        return distrax.EpsilonGreedy(preferences=q_values, epsilon=self.epsilon), q_logits, atoms


class DistributionalContinuousQNetwork(nn.Module):
    num_atoms: int
    vmin: float
    vmax: float
    kernel_init: Initializer = lecun_normal()

    @nn.compact
    def __call__(
        self, embedding: chex.Array
    ) -> Tuple[distrax.EpsilonGreedy, chex.Array, chex.Array]:
        atoms = jnp.linspace(self.vmin, self.vmax, self.num_atoms)
        q_logits = nn.Dense(self.num_atoms, kernel_init=self.kernel_init)(embedding)
        q_dist = jax.nn.softmax(q_logits)
        q_value = jnp.sum(q_dist * atoms, axis=-1)
        atoms = jnp.broadcast_to(atoms, (*q_value.shape, self.num_atoms))
        return q_value, q_logits, atoms


class QuantileDiscreteQNetwork(nn.Module):
    action_dim: int
    epsilon: float
    num_quantiles: int
    kernel_init: Initializer = lecun_normal()

    @nn.compact
    def __call__(self, embedding: chex.Array) -> Tuple[distrax.EpsilonGreedy, chex.Array]:
        q_logits = nn.Dense(self.action_dim * self.num_quantiles, kernel_init=self.kernel_init)(
            embedding
        )
        q_dist = jnp.reshape(q_logits, (-1, self.action_dim, self.num_quantiles))
        q_values = jnp.mean(q_dist, axis=-1)
        q_values = jax.lax.stop_gradient(q_values)
        return distrax.EpsilonGreedy(preferences=q_values, epsilon=self.epsilon), q_dist


class LinearHead(nn.Module):
    output_dim: int
    kernel_init: Initializer = orthogonal(0.01)

    @nn.compact
    def __call__(self, embedding: chex.Array) -> chex.Array:

        return nn.Dense(self.output_dim, kernel_init=self.kernel_init)(embedding)


class MultiDiscreteHead(nn.Module):
    """A head for multi-discrete action spaces, where each
        discrete subspace can have a different number of dimensions.

    Arguments:
        action_dim: Total number of actions across all discrete subspaces.
        number_of_dims_per_distribution: A list where each element
            represents the number of dimensions for each discrete subspace.
        kernel_init: Initializer for the dense layer.
    """

    action_dim: int
    number_of_dims_per_distribution: list[int]
    kernel_init: Initializer = orthogonal(0.01)

    @nn.compact
    def __call__(self, embedding: chex.Array) -> MultiDiscreteActionDistribution:
        assert sum(self.number_of_dims_per_distribution) == self.action_dim, (
            f"Sum of number_of_dims_per_distribution {sum(self.number_of_dims_per_distribution)} "
            f"must equal action_dim {self.action_dim}."
        )
        logits = nn.Dense(self.action_dim, kernel_init=self.kernel_init)(embedding)

        return MultiDiscreteActionDistribution(
            flat_logits=logits, number_of_dims_per_distribution=self.number_of_dims_per_distribution
        )
</file>

<file path="stoix/systems/search/search_types.py">
from typing import Callable, Dict, Tuple, Union

import chex
import mctx
from distrax import DistributionLike
from flax.core.frozen_dict import FrozenDict
from optax import OptState
from stoa import TimeStep, WrapperState
from typing_extensions import NamedTuple

from stoix.base_types import Action, ActorCriticParams, Done, Observation, Value

SearchApply = Callable[[FrozenDict, chex.PRNGKey, mctx.RootFnOutput], mctx.PolicyOutput]
RootFnApply = Callable[[FrozenDict, Observation, chex.ArrayTree, chex.PRNGKey], mctx.RootFnOutput]
EnvironmentStep = Callable[[chex.ArrayTree, Action], Tuple[chex.ArrayTree, TimeStep]]

RepresentationApply = Callable[[FrozenDict, Observation], chex.Array]
DynamicsApply = Callable[[FrozenDict, chex.Array, chex.Array], Tuple[chex.Array, DistributionLike]]


class ExItTransition(NamedTuple):
    done: Done
    action: Action
    reward: chex.Array
    search_value: Value
    search_policy: chex.Array
    obs: chex.Array
    info: Dict


class SampledExItTransition(NamedTuple):
    done: chex.Array
    action: Action
    sampled_actions: chex.Array
    reward: chex.Array
    search_value: Value
    search_policy: chex.Array
    obs: chex.Array
    info: Dict


class MZParams(NamedTuple):
    prediction_params: ActorCriticParams
    world_model_params: FrozenDict


class ZLearnerState(NamedTuple):
    params: Union[MZParams, ActorCriticParams]
    opt_states: OptState
    buffer_state: chex.ArrayTree
    key: chex.PRNGKey
    env_state: WrapperState
    timestep: TimeStep
</file>

<file path="stoix/utils/multistep.py">
from typing import Optional, Tuple, Union

import chex
import jax
import jax.numpy as jnp
from chex import Scalar
from jax import Array

# These functions are generally taken from rlax but edited to explicitly take in a batch of data.
# This is because the original rlax functions are not batched and are meant to be used with vmap,
# which can be much slower.


def batch_truncated_generalized_advantage_estimation(
    r_t: Array,
    discount_t: Array,
    lambda_: Union[Array, Scalar],
    values: Optional[Array] = None,
    v_tm1: Optional[Array] = None,
    v_t: Optional[Array] = None,
    truncation_t: Optional[Array] = None,
    stop_target_gradients: bool = False,
    time_major: bool = False,
    standardize_advantages: bool = False,
) -> Array:
    """Computes truncated generalized advantage estimates for batched sequences of length k.

    The advantages are computed in a backwards fashion according to the equation:
     =  + () *  + ... + ... + () * 
    where  = r +  * v(s) - v(s).

    See Proximal Policy Optimization Algorithms, Schulman et al.:
    https://arxiv.org/abs/1707.06347

    Note: This paper uses a different notation than the RLax standard
    convention that follows Sutton & Barto. We use r to denote the reward
    received after acting in state s, while the PPO paper uses r.

    Args:
        r_t: Rewards tensor at times [1, k] with shape [B, T] for batch-major or [T, B]
            for time-major, where B is batch size and T is the number of time steps.
        discount_t: Discount tensor at times [1, k] with the same shape as r_t.
        lambda_: Mixing parameter; a scalar or tensor at times [1, k] with the same
            shape as r_t.
        values: Values tensor at times [0, k] with shape [B, T+1] for batch-major or
        [T+1, B] for time-major. Contains one more element than r_t along the time
        dimension. If None, the v_tm1 and v_t arguments must be provided. This is if
        truncation is not used, since in truncation special bootstrap values must be
        provided. This interface is just for convenience.
        v_tm1: Values tensor at times [0, k-1] with shape [B, T] for batch-major or
            [T, B] for time-major. These are the baseline values for the current states.
            Important: These are the values to be subtracted from the r_t + v_t targets.
            Due to autoreset, these values must skip the last time step T, autoreset
            makes the timestep go as [0, 1, 2, ..., T-1, 0, 1, 2, ..., T-1, 0, 1, ...].
        v_t: Values tensor at times [1, k] with the same shape as r_t.
            These are the values for bootstrapping from next states i.e. the v_t in
            r_t + v_t - v_tm1. To correctly handle truncation, these values need to include
            the values of the final timestep T. These values do not include the first timestep 0.
            Due to autoreset, these values must skip the first time step 0, so sequences look
            like [1, 2, ..., T-1, T, 1, 2, ..., T-1, T, 1, ...].
        stop_target_gradients: bool indicating whether or not to apply stop gradient
            to targets.
        time_major: bool indicating whether the input tensors are in time-major format
            (time dimension first) or batch-major format (batch dimension first).
        standardize_advantages: bool indicating whether to standardize the advantages.
        truncation_t: Truncation indicators tensor at times [1, k] with the same shape
            as r_t, where 1 indicates a truncation point and 0 indicates a normal step.
            If None, no truncation is assumed.

    Returns:
      A tuple containing:
        - advantages: The generalized advantage estimates at times [0, k-1].
        - target_values: The target values for value function training, computed
          as values + advantages (i.e., values plus advantage estimates).
    """
    # if truncation flags are provided, we need to ensure that v_tm1 and v_t are provided
    if truncation_t is not None:
        chex.assert_type([v_tm1, v_t], float)

    # If no values are provided, we assume that v_tm1 and v_t are provided.
    # If values are provided, we use them to create v_tm1 and v_t.
    if values is None:
        chex.assert_type([v_tm1, v_t], float)
    else:
        chex.assert_rank([values], 2)
        chex.assert_type([values], float)
        if time_major:
            v_tm1 = values[:-1]
            v_t = values[1:]
        else:
            v_tm1 = values[:, :-1]
            v_t = values[:, 1:]

    chex.assert_rank([r_t, discount_t, v_tm1, v_t], 2)
    chex.assert_type([r_t, discount_t, v_tm1, v_t], float)
    chex.assert_equal_shape([r_t, v_tm1, v_t])
    lambda_ = jnp.ones_like(discount_t) * lambda_  # If scalar, make into vector.

    # Default truncation_t to all zeros if not provided
    if truncation_t is None:
        truncation_t = jnp.zeros_like(discount_t)
    else:
        chex.assert_rank([truncation_t], 2)
        chex.assert_equal_shape([truncation_t, discount_t])
        truncation_t = truncation_t.astype(float)

    if not time_major:
        r_t = jnp.transpose(r_t, (1, 0))
        discount_t = jnp.transpose(discount_t, (1, 0))
        v_tm1 = jnp.transpose(v_tm1, (1, 0))
        v_t = jnp.transpose(v_t, (1, 0))
        lambda_ = jnp.transpose(lambda_, (1, 0))
        truncation_t = jnp.transpose(truncation_t, (1, 0))

    # Use bootstrap_values directly for handling autoreset correctly
    delta_t = r_t + discount_t * v_t - v_tm1

    # Iterate backwards to calculate advantages.
    def _body(acc: Array, xs: Tuple[Array, Array, Array, Array]) -> Tuple[Array, Array]:
        deltas, discounts, lambda_, truncation = xs
        # Reset accumulator at truncation points while still using the current delta
        acc = deltas + discounts * lambda_ * acc * (1.0 - truncation)
        return acc, acc

    _, advantage_t = jax.lax.scan(
        _body,
        jnp.zeros(r_t.shape[1]),
        (delta_t, discount_t, lambda_, truncation_t),
        reverse=True,
    )

    target_values = v_tm1 + advantage_t

    if not time_major:
        advantage_t = jnp.transpose(advantage_t, (1, 0))
        target_values = jnp.transpose(target_values, (1, 0))

    if standardize_advantages:
        advantage_t = jax.nn.standardize(advantage_t, axis=(0, 1))

    if stop_target_gradients:
        advantage_t = jax.lax.stop_gradient(advantage_t)
        target_values = jax.lax.stop_gradient(target_values)

    return advantage_t, target_values


def batch_n_step_bootstrapped_returns(
    r_t: Array,
    discount_t: Array,
    v_t: Array,
    n: int,
    lambda_t: float = 1.0,
    stop_target_gradients: bool = True,
) -> Array:
    """Computes strided n-step bootstrapped return targets over a batch of sequences.

    The returns are computed according to the below equation iterated `n` times:

        G = r +  [(1 - ) v +  G].

    When lambda_t == 1. (default), this reduces to

        G = r +  * (r +  * (... * (r +  * v ))).

    Args:
        r_t: rewards at times B x [1, ..., T].
        discount_t: discounts at times B x [1, ..., T].
        v_t: state or state-action values to bootstrap from at time B x [1, ...., T].
        n: number of steps over which to accumulate reward before bootstrapping.
        lambda_t: lambdas at times B x [1, ..., T]. Shape is [], or B x [T-1].
        stop_target_gradients: bool indicating whether or not to apply stop gradient
        to targets.

    Returns:
        estimated bootstrapped returns at times B x [0, ...., T-1]
    """
    # swap axes to make time axis the first dimension
    r_t, discount_t, v_t = jax.tree_util.tree_map(
        lambda x: jnp.swapaxes(x, 0, 1), (r_t, discount_t, v_t)
    )
    seq_len = r_t.shape[0]
    batch_size = r_t.shape[1]

    # Maybe change scalar lambda to an array.
    lambda_t = jnp.ones_like(discount_t) * lambda_t

    # Shift bootstrap values by n and pad end of sequence with last value v_t[-1].
    pad_size = min(n - 1, seq_len)
    targets = jnp.concatenate([v_t[n - 1 :], jnp.array([v_t[-1]] * pad_size)], axis=0)

    # Pad sequences. Shape is now (T + n - 1,).
    r_t = jnp.concatenate([r_t, jnp.zeros((n - 1, batch_size))], axis=0)
    discount_t = jnp.concatenate([discount_t, jnp.ones((n - 1, batch_size))], axis=0)
    lambda_t = jnp.concatenate([lambda_t, jnp.ones((n - 1, batch_size))], axis=0)
    v_t = jnp.concatenate([v_t, jnp.array([v_t[-1]] * (n - 1))], axis=0)

    # Work backwards to compute n-step returns.
    for i in reversed(range(n)):
        r_ = r_t[i : i + seq_len]
        discount_ = discount_t[i : i + seq_len]
        lambda_ = lambda_t[i : i + seq_len]
        v_ = v_t[i : i + seq_len]
        targets = r_ + discount_ * ((1.0 - lambda_) * v_ + lambda_ * targets)

    targets = jnp.swapaxes(targets, 0, 1)
    return jax.lax.select(stop_target_gradients, jax.lax.stop_gradient(targets), targets)


def batch_general_off_policy_returns_from_q_and_v(
    q_t: Array,
    v_t: Array,
    r_t: Array,
    discount_t: Array,
    c_t: Array,
    stop_target_gradients: bool = False,
) -> Array:
    """Calculates targets for various off-policy evaluation algorithms.

    Given a window of experience of length `K+1`, generated by a behaviour policy
    , for each time-step `t` we can estimate the return `G_t` from that step
    onwards, under some target policy , using the rewards in the trajectory, the
    values under  of states and actions selected by , according to equation:

      G = r +  * (v - c * q(a) + c* G),

    where, depending on the choice of `c_t`, the algorithm implements:

      Importance Sampling             c_t = (x_t, a_t) / (x_t, a_t),
      Harutyunyan's et al. Q(lambda)  c_t = ,
      Precup's et al. Tree-Backup     c_t = (x_t, a_t),
      Munos' et al. Retrace           c_t =  min(1, (x_t, a_t) / (x_t, a_t)).

    See "Safe and Efficient Off-Policy Reinforcement Learning" by Munos et al.
    (https://arxiv.org/abs/1606.02647).

    Args:
      q_t: Q-values under  of actions executed by  at times [1, ..., K - 1].
      v_t: Values under  at times [1, ..., K].
      r_t: rewards at times [1, ..., K].
      discount_t: discounts at times [1, ..., K].
      c_t: weights at times [1, ..., K - 1].
      stop_target_gradients: bool indicating whether or not to apply stop gradient
        to targets.

    Returns:
      Off-policy estimates of the generalized returns from states visited at times
      [0, ..., K - 1].
    """
    q_t, v_t, r_t, discount_t, c_t = jax.tree_util.tree_map(
        lambda x: jnp.swapaxes(x, 0, 1), (q_t, v_t, r_t, discount_t, c_t)
    )

    g = r_t[-1] + discount_t[-1] * v_t[-1]  # G_K-1.

    def _body(acc: Array, xs: Tuple[Array, Array, Array, Array, Array]) -> Tuple[Array, Array]:
        reward, discount, c, v, q = xs
        acc = reward + discount * (v - c * q + c * acc)
        return acc, acc

    _, returns = jax.lax.scan(
        _body, g, (r_t[:-1], discount_t[:-1], c_t, v_t[:-1], q_t), reverse=True
    )
    returns = jnp.concatenate([returns, g[jnp.newaxis]], axis=0)

    returns = jnp.swapaxes(returns, 0, 1)
    return jax.lax.select(stop_target_gradients, jax.lax.stop_gradient(returns), returns)


def batch_retrace_continuous(
    q_tm1: Array,
    q_t: Array,
    v_t: Array,
    r_t: Array,
    discount_t: Array,
    log_rhos: Array,
    lambda_: Union[Array, float],
    stop_target_gradients: bool = True,
) -> Array:
    """Retrace continuous.

    See "Safe and Efficient Off-Policy Reinforcement Learning" by Munos et al.
    (https://arxiv.org/abs/1606.02647).

    Args:
      q_tm1: Q-values at times [0, ..., K - 1].
      q_t: Q-values evaluated at actions collected using behavior
        policy at times [1, ..., K - 1].
      v_t: Value estimates of the target policy at times [1, ..., K].
      r_t: reward at times [1, ..., K].
      discount_t: discount at times [1, ..., K].
      log_rhos: Log importance weight pi_target/pi_behavior evaluated at actions
        collected using behavior policy [1, ..., K - 1].
      lambda_: scalar or a vector of mixing parameter lambda.
      stop_target_gradients: bool indicating whether or not to apply stop gradient
        to targets.

    Returns:
      Retrace error.
    """

    c_t = jnp.minimum(1.0, jnp.exp(log_rhos)) * lambda_

    # The generalized returns are independent of Q-values and cs at the final
    # state.
    target_tm1 = batch_general_off_policy_returns_from_q_and_v(q_t, v_t, r_t, discount_t, c_t)

    target_tm1 = jax.lax.select(
        stop_target_gradients, jax.lax.stop_gradient(target_tm1), target_tm1
    )
    return target_tm1 - q_tm1


def batch_lambda_returns(
    r_t: Array,
    discount_t: Array,
    v_t: Array,
    lambda_: chex.Numeric = 1.0,
    stop_target_gradients: bool = False,
    time_major: bool = False,
) -> Array:
    """Estimates a multistep truncated lambda return from a trajectory.

    Given a a trajectory of length `T+1`, generated under some policy , for each
    time-step `t` we can estimate a target return `G_t`, by combining rewards,
    discounts, and state values, according to a mixing parameter `lambda`.

    The parameter `lambda_`  mixes the different multi-step bootstrapped returns,
    corresponding to accumulating `k` rewards and then bootstrapping using `v_t`.

        r +  v
        r +  r +   v
        r +  r +   r +    v

    The returns are computed recursively, from `G_{T-1}` to `G_0`, according to:

        G = r +  [(1 - ) v +  G].

    In the `on-policy` case, we estimate a return target `G_t` for the same
    policy  that was used to generate the trajectory. In this setting the
    parameter `lambda_` is typically a fixed scalar factor. Depending
    on how values `v_t` are computed, this function can be used to construct
    targets for different multistep reinforcement learning updates:

        TD():  `v_t` contains the state value estimates for each state under .
        Q():  `v_t = max(q_t, axis=-1)`, where `q_t` estimates the action values.
        Sarsa():  `v_t = q_t[..., a_t]`, where `q_t` estimates the action values.

    In the `off-policy` case, the mixing factor is a function of state, and
    different definitions of `lambda` implement different off-policy corrections:

        Per-decision importance sampling:   =   =  [(a|s) / (a|s)]
        V-trace, as instantiated in IMPALA:   = min(1, )

    Note that the second option is equivalent to applying per-decision importance
    sampling, but using an adaptive () = min(1/, 1), such that the effective
    bootstrap parameter at time t becomes  = () *  = min(1, ).
    This is the interpretation used in the ABQ() algorithm (Mahmood 2017).

    Of course this can be augmented to include an additional factor .  For
    instance we could use V-trace with a fixed additional parameter  = 0.9, by
    setting  = 0.9 * min(1, ) or, alternatively (but not equivalently),
     = min(0.9, ).

    Estimated return are then often used to define a td error, e.g.:  (G - v).

    See "Reinforcement Learning: An Introduction" by Sutton and Barto.
    (http://incompleteideas.net/sutton/book/ebook/node74.html).

    Args:
        r_t: sequence of rewards r for timesteps t in B x [1, T].
        discount_t: sequence of discounts  for timesteps t in B x [1, T].
        v_t: sequence of state values estimates under  for timesteps t in B x [1, T].
        lambda_: mixing parameter; a scalar or a vector for timesteps t in B x [1, T].
        stop_target_gradients: bool indicating whether or not to apply stop gradient
        to targets.
        time_major: If True, the first dimension of the input tensors is the time
        dimension.

    Returns:
        Multistep lambda returns.
    """

    chex.assert_rank([r_t, discount_t, v_t, lambda_], [2, 2, 2, {0, 1, 2}])
    chex.assert_type([r_t, discount_t, v_t, lambda_], float)
    chex.assert_equal_shape([r_t, discount_t, v_t])

    # Swap axes to make time axis the first dimension
    if not time_major:
        r_t, discount_t, v_t = jax.tree_util.tree_map(
            lambda x: jnp.swapaxes(x, 0, 1), (r_t, discount_t, v_t)
        )

    # If scalar make into vector.
    lambda_ = jnp.ones_like(discount_t) * lambda_

    # Work backwards to compute `G_{T-1}`, ..., `G_0`.
    def _body(acc: Array, xs: Tuple[Array, Array, Array, Array]) -> Tuple[Array, Array]:
        returns, discounts, values, lambda_ = xs
        acc = returns + discounts * ((1 - lambda_) * values + lambda_ * acc)
        return acc, acc

    _, returns = jax.lax.scan(_body, v_t[-1], (r_t, discount_t, v_t, lambda_), reverse=True)

    if not time_major:
        returns = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), returns)

    return jax.lax.select(stop_target_gradients, jax.lax.stop_gradient(returns), returns)


def batch_discounted_returns(
    r_t: Array,
    discount_t: Array,
    v_t: Array,
    stop_target_gradients: bool = False,
    time_major: bool = False,
) -> Array:
    """Calculates a discounted return from a trajectory.

    The returns are computed recursively, from `G_{T-1}` to `G_0`, according to:

        G = r +  G.

    See "Reinforcement Learning: An Introduction" by Sutton and Barto.
    (http://incompleteideas.net/sutton/book/ebook/node61.html).

    Args:
        r_t: reward sequence at time t.
        discount_t: discount sequence at time t.
        v_t: value sequence or scalar at time t.
        stop_target_gradients: bool indicating whether or not to apply stop gradient
        to targets.

    Returns:
        Discounted returns.
    """
    chex.assert_rank([r_t, discount_t, v_t], [2, 2, {0, 1, 2}])
    chex.assert_type([r_t, discount_t, v_t], float)

    # If scalar make into vector.
    bootstrapped_v = jnp.ones_like(discount_t) * v_t
    return batch_lambda_returns(
        r_t,
        discount_t,
        bootstrapped_v,
        lambda_=1.0,
        stop_target_gradients=stop_target_gradients,
        time_major=time_major,
    )


def importance_corrected_td_errors(
    r_t: Array,
    discount_t: Array,
    rho_tm1: Array,
    lambda_: Array,
    values: Array,
    truncation_t: Array = None,
    stop_target_gradients: bool = False,
) -> Array:
    """Computes the multistep td errors with per decision importance sampling.

    Given a trajectory of length `T+1`, generated under some policy , for each
    time-step `t` we can estimate a multistep temporal difference error (,),
    by combining rewards, discounts, and state values, according to a mixing
    parameter `` and importance sampling ratios  = (a|s) / (a|s):

      td-error =  (,)
      (,) =  +    (,),

    where  = r +  v - v is the one step, temporal difference error
    for the agent's state value estimates. This is equivalent to computing
    the -return with  =  (e.g. using the `lambda_returns` function from
    above), and then computing errors as  td-error = (G - v).

    See "A new Q() with interim forward view and Monte Carlo equivalence"
    by Sutton et al. (http://proceedings.mlr.press/v32/sutton14.html).

    Args:
      r_t: sequence of rewards r for timesteps t in [1, T].
      discount_t: sequence of discounts  for timesteps t in [1, T].
      rho_tm1: sequence of importance ratios for all timesteps t in [0, T-1].
      lambda_: mixing parameter; scalar or have per timestep values in [1, T].
      values: sequence of state values under  for all timesteps t in [0, T].
      truncation_t: sequence of truncation indicators at times [1, T], where 1 indicates
        a truncation point and 0 indicates a normal step. If None, no truncation is assumed.
      stop_target_gradients: bool indicating whether or not to apply stop gradient
        to targets.

    Returns:
      Off-policy estimates of the multistep td errors.
    """
    chex.assert_rank([r_t, discount_t, rho_tm1, values], [1, 1, 1, 1])
    chex.assert_type([r_t, discount_t, rho_tm1, values], float)
    chex.assert_equal_shape([r_t, discount_t, rho_tm1, values[1:]])

    v_tm1 = values[:-1]  # Predictions to compute errors for.
    v_t = values[1:]  # Values for bootstrapping.
    rho_t = jnp.concatenate((rho_tm1[1:], jnp.array([1.0])))  # Unused dummy value.
    lambda_ = jnp.ones_like(discount_t) * lambda_  # If scalar, make into vector.

    # Default truncation_t to all zeros if not provided
    if truncation_t is None:
        truncation_t = jnp.zeros_like(discount_t)
    else:
        chex.assert_rank([truncation_t], 1)
        chex.assert_type([truncation_t], float)
        chex.assert_equal_shape([truncation_t, discount_t])

    # Compute the one step temporal difference errors.
    one_step_delta = r_t + discount_t * v_t - v_tm1

    # Work backwards to compute `delta_{T-1}`, ..., `delta_0`.
    def _body(acc: Array, xs: Tuple[Array, Array, Array, Array, Array]) -> Tuple[Array, Array]:
        deltas, discounts, rho_t, lambda_, truncation = xs
        # Reset accumulator at truncation points while still using the current delta
        acc = deltas + discounts * rho_t * lambda_ * acc * (1.0 - truncation)
        return acc, acc

    _, errors = jax.lax.scan(
        _body,
        0.0,
        (one_step_delta, discount_t, rho_t, lambda_, truncation_t),
        reverse=True,
    )

    errors = rho_tm1 * errors
    return jax.lax.select(
        stop_target_gradients, jax.lax.stop_gradient(errors + v_tm1) - v_tm1, errors
    )


def batch_q_lambda(
    r_t: chex.Array,
    discount_t: chex.Array,
    q_t: chex.Array,
    lambda_: chex.Numeric,
    stop_target_gradients: bool = True,
    time_major: bool = False,
) -> chex.Array:
    """Calculates Peng's or Watkins' Q(lambda) returns.

    See "Reinforcement Learning: An Introduction" by Sutton and Barto.
    (http://incompleteideas.net/book/ebook/node78.html).

    Args:
        r_t: sequence of rewards at time t.
        discount_t: sequence of discounts at time t.
        q_t: sequence of Q-values at time t.
        lambda_: mixing parameter lambda, either a scalar (e.g. Peng's Q(lambda)) or
        a sequence (e.g. Watkin's Q(lambda)).
        stop_target_gradients: bool indicating whether or not to apply stop gradient
        to targets.
        time_major: If True, the first dimension of the input tensors is the time.

    Returns:
        Q(lambda) target values.
    """
    chex.assert_rank([r_t, discount_t, q_t, lambda_], [2, 2, 3, {0, 1, 2}])
    chex.assert_type([r_t, discount_t, q_t, lambda_], [float, float, float, float])
    v_t = jnp.max(q_t, axis=-1)
    target_tm1 = batch_lambda_returns(
        r_t, discount_t, v_t, lambda_, stop_target_gradients, time_major=time_major
    )

    target_tm1 = jax.lax.select(
        stop_target_gradients, jax.lax.stop_gradient(target_tm1), target_tm1
    )
    return target_tm1
</file>

<file path="stoix/configs/arch/anakin.yaml">
# --- Anakin config ---
architecture_name: anakin
# --- Training ---
seed: 42  # RNG seed.
update_batch_size: 1 # Number of vectorised gradient updates per device.
total_num_envs: 1024  # Total Number of vectorised environments across all devices and batched_updates. Needs to be divisible by n_devices*update_batch_size.
total_timesteps: 1e7 # Set the total environment steps.
# If unspecified, it's derived from num_updates; otherwise, num_updates adjusts based on this value.
num_updates: ~ # Number of updates

# --- Evaluation ---
evaluation_greedy: False # Evaluate the policy greedily. If True the policy will select
  # an action which corresponds to the greatest logit. If false, the policy will sample
  # from the logits.
num_eval_episodes: 128 # Number of episodes to evaluate per evaluation.
num_evaluation: 20 # Number of evenly spaced evaluations to perform during training.
absolute_metric: True # Whether the absolute metric should be computed. For more details
  # on the absolute metric please see: https://arxiv.org/abs/2209.10485
</file>

<file path="stoix/networks/base.py">
import functools
from typing import Any, Dict, List, Optional, Sequence, Tuple, Union

import chex
import distrax
import hydra
import jax
import jax.numpy as jnp
import numpy as np
from flax import linen as nn

from stoix.base_types import Observation, RNNObservation
from stoix.networks.inputs import ArrayInput
from stoix.networks.utils import parse_rnn_cell


class FeedForwardActor(nn.Module):
    """Simple Feedforward Actor Network."""

    action_head: nn.Module
    torso: nn.Module
    input_layer: nn.Module = ArrayInput()

    @nn.compact
    def __call__(
        self,
        observation: Observation,
        input_kwargs: Optional[Dict] = None,
        torso_kwargs: Optional[Dict] = None,
        head_kwargs: Optional[Dict] = None,
    ) -> distrax.DistributionLike:

        obs_embedding = self.input_layer(observation, **(input_kwargs or {}))
        obs_embedding = self.torso(obs_embedding, **(torso_kwargs or {}))
        return self.action_head(obs_embedding, **(head_kwargs or {}))


class FeedForwardCritic(nn.Module):
    """Simple Feedforward Critic Network."""

    critic_head: nn.Module
    torso: nn.Module
    input_layer: nn.Module = ArrayInput()

    @nn.compact
    def __call__(
        self,
        observation: Observation,
        input_kwargs: Optional[Dict] = None,
        torso_kwargs: Optional[Dict] = None,
        head_kwargs: Optional[Dict] = None,
    ) -> chex.Array:

        obs_embedding = self.input_layer(observation, **(input_kwargs or {}))
        obs_embedding = self.torso(obs_embedding, **(torso_kwargs or {}))
        critic_output = self.critic_head(obs_embedding, **(head_kwargs or {}))

        return critic_output


class FeedForwardActorCritic(nn.Module):
    """Simple Feedforward Joint Actor Critic Network."""

    action_head: nn.Module
    critic_head: nn.Module
    torso: nn.Module
    input_layer: nn.Module = ArrayInput()

    @nn.compact
    def __call__(
        self,
        observation: Observation,
        input_kwargs: Optional[Dict] = None,
        torso_kwargs: Optional[Dict] = None,
        actor_head_kwargs: Optional[Dict] = None,
        critic_head_kwargs: Optional[Dict] = None,
    ) -> chex.Array:

        obs_embedding = self.input_layer(observation, **(input_kwargs or {}))
        obs_embedding = self.torso(obs_embedding, **(torso_kwargs or {}))
        actor_output = self.action_head(obs_embedding, **(actor_head_kwargs or {}))
        critic_output = self.critic_head(obs_embedding, **(critic_head_kwargs or {}))

        return actor_output, critic_output


class CompositeNetwork(nn.Module):
    """Composite Network. Takes in a sequence of layers and applies them sequentially."""

    layers: Sequence[nn.Module]

    @nn.compact
    def __call__(
        self, *network_input: Union[chex.Array, Tuple[chex.Array, ...]]
    ) -> Union[distrax.DistributionLike, chex.Array]:

        x = self.layers[0](*network_input)
        for layer in self.layers[1:]:
            x = layer(x)
        return x


class MultiNetwork(nn.Module):
    """Multi Network.

    Takes in a sequence of networks, applies them separately and concatenates the outputs."""

    networks: Sequence[nn.Module]

    @nn.compact
    def __call__(
        self, *network_input: Union[chex.Array, Tuple[chex.Array, ...]]
    ) -> Union[distrax.DistributionLike, chex.Array]:
        """Forward pass."""
        outputs = []
        for network in self.networks:
            outputs.append(network(*network_input))
        concatenated = jnp.stack(outputs, axis=-1)
        chex.assert_rank(concatenated, 2)
        return concatenated


class ScannedRNN(nn.Module):
    hidden_state_dim: int
    cell_type: str

    @functools.partial(
        nn.scan,
        variable_broadcast="params",
        in_axes=0,
        out_axes=0,
        split_rngs={"params": False},
    )
    @nn.compact
    def __call__(self, rnn_state: chex.Array, x: chex.Array) -> Tuple[chex.Array, chex.Array]:
        """Applies the module."""
        ins, resets = x
        hidden_state_reset_fn = lambda reset_state, current_state: jnp.where(
            resets[:, np.newaxis],
            reset_state,
            current_state,
        )
        rnn_state = jax.tree_util.tree_map(
            hidden_state_reset_fn,
            self.initialize_carry(ins.shape[0]),
            rnn_state,
        )
        new_rnn_state, y = parse_rnn_cell(self.cell_type)(features=self.hidden_state_dim)(
            rnn_state, ins
        )
        return new_rnn_state, y

    @nn.nowrap
    def initialize_carry(self, batch_size: int) -> chex.Array:
        """Initializes the carry state."""
        # Use a dummy key since the default state init fn is just zeros.
        cell = parse_rnn_cell(self.cell_type)(features=self.hidden_state_dim)
        return cell.initialize_carry(jax.random.PRNGKey(0), (batch_size, self.hidden_state_dim))


class RecurrentActor(nn.Module):
    """Recurrent Actor Network."""

    action_head: nn.Module
    post_torso: nn.Module
    hidden_state_dim: int
    cell_type: str
    pre_torso: nn.Module
    input_layer: nn.Module = ArrayInput()

    @nn.compact
    def __call__(
        self,
        policy_hidden_state: chex.Array,
        observation_done: RNNObservation,
    ) -> Tuple[chex.Array, distrax.DistributionLike]:

        observation, done = observation_done

        observation = self.input_layer(observation)
        policy_embedding = self.pre_torso(observation)
        policy_rnn_input = (policy_embedding, done)
        policy_hidden_state, policy_embedding = ScannedRNN(self.hidden_state_dim, self.cell_type)(
            policy_hidden_state, policy_rnn_input
        )
        actor_logits = self.post_torso(policy_embedding)
        pi = self.action_head(actor_logits)

        return policy_hidden_state, pi


class RecurrentCritic(nn.Module):
    """Recurrent Critic Network."""

    critic_head: nn.Module
    post_torso: nn.Module
    hidden_state_dim: int
    cell_type: str
    pre_torso: nn.Module
    input_layer: nn.Module = ArrayInput()

    @nn.compact
    def __call__(
        self,
        critic_hidden_state: Tuple[chex.Array, chex.Array],
        observation_done: RNNObservation,
    ) -> Tuple[chex.Array, chex.Array]:

        observation, done = observation_done

        observation = self.input_layer(observation)

        critic_embedding = self.pre_torso(observation)
        critic_rnn_input = (critic_embedding, done)
        critic_hidden_state, critic_embedding = ScannedRNN(self.hidden_state_dim, self.cell_type)(
            critic_hidden_state, critic_rnn_input
        )
        critic_output = self.post_torso(critic_embedding)
        critic_output = self.critic_head(critic_output)

        return critic_hidden_state, critic_output


def chained_torsos(torso_cfgs: List[Dict[str, Any]]) -> nn.Module:
    """Create a network by chaining multiple torsos together using a list of configs.
    This makes use of hydra to instantiate the modules and the composite network
    to chain them together.

    Args:
        torso_cfgs: List of dictionaries containing the configuration for each torso.
            These configs should use the same format as the individual torso configs."""

    torso_modules = [hydra.utils.instantiate(torso_cfg) for torso_cfg in torso_cfgs]
    return CompositeNetwork(torso_modules)
</file>

<file path="stoix/networks/torso.py">
from typing import Sequence

import chex
import numpy as np
from flax import linen as nn
from flax.linen.initializers import Initializer, orthogonal

from stoix.networks.layers import NoisyLinear
from stoix.networks.utils import parse_activation_fn


class MLPTorso(nn.Module):
    """MLP torso."""

    layer_sizes: Sequence[int]
    activation: str = "relu"
    use_layer_norm: bool = False
    kernel_init: Initializer = orthogonal(np.sqrt(2.0))
    activate_final: bool = True

    @nn.compact
    def __call__(self, observation: chex.Array) -> chex.Array:
        """Forward pass."""
        x = observation
        for layer_size in self.layer_sizes:
            x = nn.Dense(
                layer_size, kernel_init=self.kernel_init, use_bias=not self.use_layer_norm
            )(x)
            if self.use_layer_norm:
                x = nn.LayerNorm()(x)
            if self.activate_final or layer_size != self.layer_sizes[-1]:
                x = parse_activation_fn(self.activation)(x)
        return x


class NoisyMLPTorso(nn.Module):
    """MLP torso using NoisyLinear layers instead of standard Dense layers."""

    layer_sizes: Sequence[int]
    activation: str = "relu"
    use_layer_norm: bool = False
    kernel_init: Initializer = orthogonal(np.sqrt(2.0))
    activate_final: bool = True
    sigma_zero: float = 0.5

    @nn.compact
    def __call__(self, observation: chex.Array) -> chex.Array:
        x = observation
        for layer_size in self.layer_sizes:
            x = NoisyLinear(
                layer_size, sigma_zero=self.sigma_zero, use_bias=not self.use_layer_norm
            )(x)
            if self.use_layer_norm:
                x = nn.LayerNorm()(x)
            if self.activate_final or layer_size != self.layer_sizes[-1]:
                x = parse_activation_fn(self.activation)(x)
        return x


class CNNTorso(nn.Module):
    """2D CNN torso. Expects input of shape (batch, height, width, channels).
    After this torso, the output is flattened and put through an MLP of
    hidden_sizes."""

    channel_sizes: Sequence[int]
    kernel_sizes: Sequence[int]
    strides: Sequence[int]
    activation: str = "relu"
    use_layer_norm: bool = False
    kernel_init: Initializer = orthogonal(np.sqrt(2.0))
    channel_first: bool = False
    hidden_sizes: Sequence[int] = (256,)

    @nn.compact
    def __call__(self, observation: chex.Array) -> chex.Array:
        """Forward pass."""
        x = observation

        # If there is a batch of sequences of images
        if observation.ndim > 4:
            return nn.batch_apply.BatchApply(self.__call__)(observation)

        # If the input is in the form of [B, C, H, W], we need to transpose it to [B, H, W, C]
        if self.channel_first:
            x = x.transpose((0, 2, 3, 1))

        # Convolutional layers
        for channel, kernel, stride in zip(self.channel_sizes, self.kernel_sizes, self.strides):
            x = nn.Conv(
                channel, (kernel, kernel), (stride, stride), use_bias=not self.use_layer_norm
            )(x)
            if self.use_layer_norm:
                x = nn.LayerNorm(reduction_axes=(-3, -2, -1))(x)
            x = parse_activation_fn(self.activation)(x)

        # Flatten
        x = x.reshape(*observation.shape[:-3], -1)

        # MLP layers
        x = MLPTorso(
            layer_sizes=self.hidden_sizes,
            activation=self.activation,
            use_layer_norm=self.use_layer_norm,
            kernel_init=self.kernel_init,
            activate_final=True,
        )(x)

        return x
</file>

<file path="stoix/systems/spo/ff_spo.py">
import copy
import functools
import time
from typing import Any, Callable, Dict, NamedTuple, Tuple

import chex
import flashbax as fbx
import flax
import hydra
import jax
import jax.numpy as jnp
import mctx
import optax
import rlax
import tensorflow_probability.substrates.jax as tfp
from colorama import Fore, Style
from flashbax.buffers.trajectory_buffer import BufferState
from flax.core.frozen_dict import FrozenDict
from omegaconf import DictConfig, OmegaConf
from stoa import Environment, TimeStep, WrapperState, get_final_step_metrics
from tensorflow_probability.substrates.jax.distributions import Distribution

from stoix.base_types import (
    Action,
    ActorApply,
    AnakinExperimentOutput,
    CriticApply,
    LearnerFn,
    OffPolicyLearnerState,
    OnlineAndTarget,
)
from stoix.networks.base import FeedForwardActor as Actor
from stoix.networks.base import FeedForwardCritic as Critic
from stoix.systems.mpo.continuous_loss import (
    compute_cross_entropy_loss,
    compute_nonparametric_kl_from_normalized_weights,
    compute_parametric_kl_penalty_and_dual_loss,
    compute_weights_and_temperature_loss,
)
from stoix.systems.mpo.discrete_loss import clip_categorical_mpo_params
from stoix.systems.mpo.mpo_types import CategoricalDualParams
from stoix.systems.search.evaluator import search_evaluator_setup
from stoix.systems.search.search_types import EnvironmentStep
from stoix.systems.spo.spo_types import (
    _SPO_FLOAT_EPSILON,
    SPOApply,
    SPOOptStates,
    SPOOutput,
    SPOParams,
    SPORecurrentFn,
    SPORecurrentFnOutput,
    SPORootFnApply,
    SPORootFnOutput,
    SPOTransition,
)
from stoix.utils import make_env as environments
from stoix.utils.checkpointing import Checkpointer
from stoix.utils.jax_utils import (
    merge_leading_dims,
    unreplicate_batch_dim,
    unreplicate_n_dims,
)
from stoix.utils.logger import LogEvent, StoixLogger
from stoix.utils.multistep import batch_truncated_generalized_advantage_estimation
from stoix.utils.total_timestep_checker import check_total_timesteps
from stoix.utils.training import make_learning_rate

tfd = tfp.distributions


def check_distribution_type(action_distribution: Distribution) -> None:
    """Verify that the policy's action distribution is of required types.

    This function ensures that the action distribution follows the expected structure
    for the SPO algorithm, which requires a Categorical distribution.

    Args:
        action_distribution: The distribution to check, typically produced by the policy.

    Raises:
        ValueError: If the distribution is not a Categorical distribution.
    """
    if not isinstance(action_distribution, tfd.Categorical):
        raise ValueError("Action distribution must be a Categorical distribution.")


def broadcast_tree(struct: chex.ArrayTree, add_dims: Tuple[int], axis: int = 0) -> chex.ArrayTree:
    """Add dimensions to each array in a tree structure and broadcast values along those dimensions.

    This function takes a PyTree of arrays and adds additional dimensions at the specified axis,
    then broadcasts the original values across the newly added dimensions.

    Args:
        struct: A PyTree of arrays to be broadcasted.
        add_dims: A tuple specifying the dimensions to add at the specified axis.
        axis: The axis position where new dimensions should be inserted. Default is 0.

    Returns:
        A new PyTree with the same structure but with arrays that have been expanded
        and broadcasted along the new dimensions.
    """

    def broadcast_fn(x: chex.Array) -> chex.Array:
        x_shape = x.shape
        prefix = x_shape[:axis]
        suffix = x_shape[axis:]
        new_shape = prefix + add_dims + suffix
        x = jnp.expand_dims(x, axis=axis)
        return jnp.broadcast_to(x, new_shape)

    return jax.tree_util.tree_map(
        broadcast_fn,
        jax.tree_util.tree_map(jnp.asarray, struct),
    )


def apply_exploration_noise(
    rng_key: chex.PRNGKey,
    prior_logits: chex.Array,
    dirichlet_alpha: float,
    dirichlet_fraction: float,
) -> chex.Array:
    """Applies Dirichlet exploration noise to a discrete distribution.

    Args:
        rng_key: Random key for generating noise.
        prior_logits: Logits of the prior policy.
        dirichlet_alpha: Alpha parameter for the Dirichlet distribution.
        dirichlet_fraction: Fraction of the action space to perturb.

    Returns:
        Noisy prior logits.
    """
    noisy_prior = rlax.add_dirichlet_noise(
        rng_key, prior_logits, dirichlet_alpha, dirichlet_fraction
    )
    return noisy_prior


def make_root_fn(
    actor_apply_fn: ActorApply,
    critic_apply_fn: CriticApply,
    config: DictConfig,
) -> SPORootFnApply:
    """Create the root function for initializing SPO search.

    This function returns a callable that generates the root objects needed for
    Sequential Monte Carlo (SMC) search. The root function samples initial actions,
    computes their values and log probabilities, and prepares environment states for
    particle-based simulation.

    Args:
        actor_apply_fn: Function for applying the actor network to observations.
        critic_apply_fn: Function for applying the critic network to observations.
        config: Configuration dictionary containing search parameters.

    Returns:
        A function that takes parameters, observation, environment state, and an RNG key,
        and returns a SPORootFnOutput containing initial particle states and actions.
    """

    def root_fn(
        params: SPOParams,
        observation: chex.ArrayTree,
        env_state: chex.ArrayTree,
        rng_key: chex.PRNGKey,
    ) -> SPORootFnOutput:
        """Initialize the root node for Sequential Monte Carlo (SMC) search.

        This function creates the starting point for SMC search by sampling actions from
        the current policy, evaluating their values, and preparing environment states for
        particle-based simulation.

        Args:
            params: Network parameters including actor, critic, and dual parameters.
            observation: Current environment observation.
            env_state: Current environment state.
            rng_key: Random key for stochastic operations.

        Returns:
            SPORootFnOutput containing initial particle states, actions, values and logits.
        """

        sample_key, noise_key = jax.random.split(rng_key, 2)
        # Run the actor and critic network on the current observation
        pi = actor_apply_fn(params.actor_params.online, observation)
        prior_logits = pi.logits
        noisy_prior_logits = apply_exploration_noise(
            noise_key,
            prior_logits,
            config.system.root_exploration_dirichlet_alpha,
            config.system.root_exploration_dirichlet_fraction,
        )
        value = critic_apply_fn(params.critic_params.online, observation)

        # Create a categorical distribution from the noisy prior logits
        noisy_pi = type(pi)(logits=noisy_prior_logits)
        # Sample an action for every particle that is going to be used in the SMC search
        sampled_actions = noisy_pi.sample(seed=sample_key, sample_shape=config.system.num_particles)
        # Swap num samples and batch dimension
        sampled_actions = jnp.swapaxes(sampled_actions, 0, 1)
        # Check shapes for sanity
        batch_size = value.shape[0]
        chex.assert_shape(
            sampled_actions,
            (
                batch_size,
                config.system.num_particles,
            ),
        )

        # We then broadcast the environment state so each particle has a copy and we do the same
        # for the critic value. This is all on the axis=1 so the shape is (batch, num particles, *)
        particle_env_states = broadcast_tree(env_state, (config.system.num_particles,), axis=1)
        value = broadcast_tree(value, (config.system.num_particles,), axis=1)
        # For each sampled action, we get the log prob from the prior policy.
        # This is not strictly necessary to have.
        log_probs = jax.vmap(pi.log_prob, in_axes=1, out_axes=1)(sampled_actions)

        # We create the root object
        root_fn_output = SPORootFnOutput(
            particle_logits=log_probs,
            particle_actions=sampled_actions,
            particle_values=value,
            particle_env_states=particle_env_states,
        )

        return root_fn_output

    return root_fn


def make_recurrent_fn(
    environment_step: EnvironmentStep,
    actor_apply_fn: ActorApply,
    critic_apply_fn: CriticApply,
    config: DictConfig,
) -> SPORecurrentFn:
    """Create the recurrent function for advancing particles during SMC search.

    This function returns a callable that steps particles through the environment model
    during the Sequential Monte Carlo (SMC) search. For each particle, it advances
    the environment state, computes rewards and values, and samples next actions.

    Args:
        environment_step: Function for stepping the environment model forward.
        actor_apply_fn: Function for applying the actor network to observations.
        critic_apply_fn: Function for applying the critic network to observations.
        config: Configuration dictionary containing search parameters.

    Returns:
        A function that takes parameters, RNG key, particle actions, and environment states,
        and returns updated states and recurrent function outputs for each particle.
    """

    def check_shapes(
        recurrent_fn_output: SPORecurrentFnOutput, particle_actions: chex.Array
    ) -> None:
        """Verify that recurrent function outputs have the expected shapes.

        This function performs shape assertions to ensure that the outputs from the
        recurrent function match the expected shapes based on the configuration.

        Args:
            recurrent_fn_output: Output from the recurrent function step.
            particle_actions: Actions taken by the particles.

        Raises:
            AssertionError: If any of the shapes don't match expectations.
        """
        chex.assert_shape(recurrent_fn_output.reward, (config.system.num_particles,))
        chex.assert_shape(recurrent_fn_output.discount, (config.system.num_particles,))
        chex.assert_shape(recurrent_fn_output.prior_logits, (config.system.num_particles,))
        chex.assert_shape(recurrent_fn_output.value, (config.system.num_particles,))
        chex.assert_shape(
            recurrent_fn_output.next_sampled_action,
            (config.system.num_particles,),
        )

    def recurrent_fn(
        params: SPOParams,
        rng_key: chex.PRNGKey,
        particle_actions: chex.Array,
        env_state: chex.ArrayTree,
    ) -> Tuple[SPORecurrentFnOutput, chex.ArrayTree]:
        """Execute one step of the environment for each particle during SMC search.

        This function advances the environment state for each particle using its selected action,
        computes new values and rewards, and samples new actions for the next state. It is designed
        to be vmapped across the batch dimension during search.

        Args:
            params: Network parameters including actor, critic, and dual parameters.
            rng_key: Random key for stochastic operations.
            particle_actions: Actions for each particle to take in the environment.
            env_state: Current environment state for each particle.

        Returns:
            A tuple containing:
                - SPORecurrentFnOutput with rewards, discounts, value estimates and next actions.
                - Updated environment state after taking the action.
        """

        # Take a step in the environment using the particles action
        # This environment step is vmapped in the learner setup.
        next_env_state, next_timestep = environment_step(env_state, particle_actions)
        # For each particle, run the actor and critic networks
        pi = actor_apply_fn(params.actor_params.online, next_timestep.observation)
        value = critic_apply_fn(params.critic_params.online, next_timestep.observation)
        # Sample a new action for the new state for each particle
        next_sampled_actions = pi.sample(seed=rng_key)
        # Check shape for sanity
        chex.assert_shape(next_sampled_actions, (config.system.num_particles,))
        # Check to see if the environment truncated
        truncated_step = next_timestep.last() & (next_timestep.discount != 0.0)
        # We set the discount to 0.0 if the environment truncated
        # The reason for this is to have an indication of if a particle terminated one way or
        # another. However, to still utilise the value for bootstrapping, we manually discount
        # the value by the correct discount in the recurrent_fn_output.
        rec_fn_discount = next_timestep.discount * (1 - truncated_step.astype(jnp.float32))
        # For the bootstrap value, we use the real discount and multiply it by the gamma here.
        bootstrap_value = next_timestep.discount * config.system.search_gamma * value
        # Get the log probabilities of the next sampled actions
        next_log_probs = pi.log_prob(next_sampled_actions)

        # Create the recurrent_fn_output
        recurrent_fn_output = SPORecurrentFnOutput(
            reward=next_timestep.reward,
            discount=rec_fn_discount,
            prior_logits=next_log_probs,
            value=bootstrap_value,
            next_sampled_action=next_sampled_actions,
        )
        # Check shapes for sanity
        check_shapes(recurrent_fn_output, particle_actions)

        return recurrent_fn_output, next_env_state

    return recurrent_fn


class Particles(NamedTuple):
    """Container for particle states used in Sequential Monte Carlo (SMC) search.

    This class stores the state of all particles being simulated during the SMC search.
    Each field has the leading dimensions [NumEnvs, NumParticles, ...] for batch processing.

    Attributes:
        state_embedding: Environment states for each particle.
        root_actions: Actions sampled at the root for each particle.
        resample_td_weights: Temporal difference weights used for resampling decisions.
        prior_logits: Log probabilities of each particle's action under the current policy.
        value: Value estimates for each particle's state.
        terminal: Boolean flags indicating if particles have reached terminal states.
        depth: The depth/step count of each particle in its trajectory.
        gae: Generalized Advantage Estimates for each particle.
    """

    state_embedding: chex.ArrayTree
    root_actions: chex.Array
    resample_td_weights: chex.Array
    prior_logits: chex.Array
    value: chex.Array
    terminal: chex.Array
    depth: chex.Array
    gae: chex.Array


class SPO:
    """Sequential Policy Optimization Search.

    This class implements the Sequential Monte Carlo (SMC) search process for the
    Sequential Policy Optimization (SPO) algorithm. It manages the creation,
    advancement, and resampling of particles to explore possible action trajectories
    and improve policy performance.

    The search process involves:
    1. Initializing particles from a root state
    2. Advancing particles through the environment model
    3. Resampling particles based on their performance
    4. Selecting the best actions based on the search results

    Attributes:
        config: Configuration parameters for the SPO algorithm.
        recurrent_fn: Function used to step particles through the environment.
    """

    def __init__(
        self,
        config: DictConfig,
        recurrent_fn: mctx.RecurrentFn,
    ):
        self.config = config
        self.recurrent_fn = recurrent_fn

    def search(
        self,
        params: SPOParams,
        rng_key: chex.PRNGKey,
        root: SPORootFnOutput,
    ) -> SPOOutput:
        """
        Perform a Sequential Monte Carlo (SMC) search to explore possible action trajectories.

        Args:
            params (SPOParams): Parameters containing actor, critic, and dual network parameters.
            rng_key (chex.Array): Random number generator key for stochastic operations.
            root (SPORootFnOutput): Output from the root function, including initial state
                embeddings and sampled actions.

        Returns:
            SPOOutput: The result of the search, including the final set of actions,
            the weights associated with these actions, the mean value over the actions,
            and advantages associated with the "initial" (not final) set of sampled actions to use
            for optimising the dual temperature parameter.
        """

        # Determine the number of parallel environments (batch size)
        batch_size = root.particle_values.shape[0]
        rng_key, rollout_key = jax.random.split(rng_key, num=2)

        rng_keys = jax.random.split(rng_key, batch_size)

        # Execute the SMC rollout to generate particle trajectories
        particles, rollout_metrics, last_resample = self.rollout(
            params,
            root,
            rollout_key,
        )

        def readout_weighted(data: Tuple[Particles, SPORootFnOutput, chex.Array]) -> SPOOutput:
            """Select action from particle set using temperature-scaled weights."""
            particles, root, rng_key = data

            if self.config.system.temperature.adaptive:
                normalised_action_logits = self.get_resample_logits(
                    particles.resample_td_weights,
                    log_temperature=params.dual_params.log_temperature,
                )
            else:
                normalised_action_logits = self.get_resample_logits(
                    particles.resample_td_weights,
                    temperature=self.config.system.temperature.fixed_temperature,
                )

            action_index = jax.random.categorical(rng_key, logits=normalised_action_logits)
            action_weights = jax.nn.softmax(normalised_action_logits, axis=-1)
            action = particles.root_actions[action_index]

            output = SPOOutput(
                action=action,
                sampled_action_weights=action_weights,
                sampled_actions=particles.root_actions,
                value=jnp.mean(root.particle_values, axis=-1),
                sampled_advantages=particles.gae,
                rollout_metrics=rollout_metrics,
            )

            return output

        output: SPOOutput = jax.vmap(readout_weighted, in_axes=(0))(
            (particles, root, rng_keys),
        )

        return output

    def rollout(
        self,
        params: SPOParams,
        root: SPORootFnOutput,
        rng_key: chex.PRNGKey,
    ) -> Tuple[Particles, Dict[str, chex.Array], chex.Array]:
        """
        Execute a Sequential Monte Carlo (SMC) rollout to explore action trajectories.

        This process involves:
            1. Initializing particles from the root state.
            2. Iteratively advancing particles through the environment using the recurrent function.
            3. Applying resampling based on temporal difference (TD) weights at specified intervals.
            4. Accumulating metrics such as values and advantages across particles.

        Args:
            params (SPOParams): Parameters containing actor, critic, and dual network parameters.
            root (SPORootFnOutput): Output from the root function, including initial state
                embeddings and actions.
            rng_key (chex.PRNGKey): Random number generator key for stochastic operations.

        Returns:
            Particles: The final state of all particles after the rollout, including embeddings,
                weights, values, and metrics.

        Note:
            - The number of rollout steps is determined by `config.system.search_depth`.
            - Each particle represents a potential trajectory through states and actions.
        """

        keys = jax.random.split(rng_key, self.config.system.search_depth)

        # Initialize particles from the root state
        initial_particles = self.init_particles(root)
        initial_sampled_actions = initial_particles.root_actions
        carry = (initial_particles, initial_sampled_actions)
        # Scan over depth and record ESS (effective sample size) at each step.
        final_carry, scan_metrics = jax.lax.scan(
            functools.partial(
                self.one_step_rollout,
                params=params,
            ),
            init=carry,
            xs=(jnp.arange(self.config.system.search_depth), keys),
        )
        (particles, _) = final_carry

        # Process the accumulated metrics from the scan
        rollout_metrics = {}
        num_steps = scan_metrics["ess"].shape[0]
        for d in range(1, num_steps + 1):
            rollout_metrics[f"ess_fraction_depth:{d}"] = (
                scan_metrics["ess"][d - 1] / self.config.system.num_particles
            )
            rollout_metrics[f"entropy_depth:{d}"] = scan_metrics["entropy"][d - 1]
            rollout_metrics[f"mean_td_weights_depth:{d}"] = scan_metrics["mean_td_weights"][d - 1]
            rollout_metrics[f"particles_alive_depth:{d}"] = scan_metrics["particles_alive"][d - 1]
            rollout_metrics[f"resample_depth:{d}"] = scan_metrics["resample"][d - 1]

        last_resample = scan_metrics["resample"][-1]

        return particles, rollout_metrics, last_resample  # type: ignore

    def one_step_rollout(
        self,
        particles_and_actions: Tuple[Particles, Action],
        depth_count_and_key: Tuple[chex.Array, chex.PRNGKey],
        params: SPOParams,
    ) -> Tuple[Tuple[Particles, Action], Dict[str, chex.Array]]:
        """
        Execute a single step of the SMC rollout process.

        This involves:
            1. Advancing all particles by one action step using the recurrent function.
            2. Updating temporal difference (TD) weights based on rewards and value estimates.
            3. Conditionally resampling particles based on the resampling period.

        Args:
            particles_and_actions (Tuple[Particles, Action]):
                - particles: Current particle states and their metrics.
                - sampled_actions: Actions sampled for each particle.
            depth_count_and_key (Tuple[chex.Array, chex.PRNGKey]):
                - current_depth: Current depth level in the rollout.
                - key: RNG key for stochastic operations.
            params (SPOParams): Parameters containing actor, critic, and dual network parameters.

        Returns:
            Tuple containing:
                - Updated (particles, sampled_actions) tuple for the next step.
                - None (placeholder for JAX scan compatibility).
        """

        # Unpack the current particles and actions
        particles, sampled_actions = particles_and_actions

        # Ensure the sampled actions have the correct shape
        chex.assert_shape(
            sampled_actions,
            (
                particles.value.shape[0],
                self.config.system.num_particles,
            ),
        )

        # Unpack the current depth and RNG key
        current_depth, key = depth_count_and_key

        # Split the RNG key for resampling and recurrent steps
        key_resampling, recurrent_step_key = jax.random.split(key)
        batch_recurrent_step_keys = jax.random.split(recurrent_step_key, particles.value.shape[0])

        # Advance the environment by one step for all particles using the recurrent function
        recurrent_output, next_state_embedding = jax.vmap(
            self.recurrent_fn, in_axes=(None, 0, 0, 0), out_axes=(0, 0)
        )(
            params,
            batch_recurrent_step_keys,
            sampled_actions,
            particles.state_embedding,
        )
        next_sampled_actions = recurrent_output.next_sampled_action

        # Update temporal difference (TD) weights based on rewards and value estimates
        updated_td_weights = self.smc_weight_update_fn(
            particles=particles,
            recurrent_output=recurrent_output,
        )
        # Get the terminal mask for the particles for logging
        particles_alive = 1 - particles.terminal.astype(jnp.int32)

        # --- Compute ESS before resampling ---
        if self.config.system.temperature.adaptive:
            ess, entropy = self.calculate_ess_and_entropy(
                updated_td_weights, log_temperature=params.dual_params.log_temperature
            )
        else:
            ess, entropy = self.calculate_ess_and_entropy(
                updated_td_weights, temperature=self.config.system.temperature.fixed_temperature
            )

        root_action = jnp.where(
            current_depth == 0,
            sampled_actions,
            particles.root_actions,
        )

        # Update particle states with new embeddings, weights, actions, and metrics
        updated_particles = self.update_particles(
            next_state_embedding,
            updated_td_weights,
            root_action,
            recurrent_output,
            particles,
        )

        # Compute logits for resampling based on updated TD weights and temperature
        if self.config.system.temperature.adaptive:
            resample_logits = self.get_resample_logits(  # Fix this if we want a fixed temperature
                updated_td_weights,
                log_temperature=params.dual_params.log_temperature,
            )
        else:
            resample_logits = self.get_resample_logits(  # Fix this if we want a fixed temperature
                updated_td_weights,
                temperature=self.config.system.temperature.fixed_temperature,
            )

        # Decide whether to resample based on the configured mode.
        resampling_mode = self.config.system.resampling.mode

        # Calculate metrics to return
        step_metrics = {
            "ess": ess,
            "entropy": entropy,
            "mean_td_weights": updated_td_weights.mean(axis=-1),
            "particles_alive": particles_alive.mean(axis=-1),
        }

        if resampling_mode == "period":
            # Check if (current_depth+1) satisfies the period condition.
            should_resample = ((current_depth + 1) % self.config.system.resampling.period) == 0

            batch_size = updated_particles.root_actions.shape[0]
            step_metrics["resample"] = should_resample.repeat(batch_size)

            # Conditionally resample particles if the resampling period is met
            updated_particles = jax.lax.cond(
                should_resample,
                lambda _: self.resample(updated_particles, key_resampling, resample_logits),
                lambda _: updated_particles,
                None,
            )

            return (updated_particles, next_sampled_actions), step_metrics

        elif resampling_mode == "ess":
            # Resample if the ESS fraction is below the provided threshold.
            # Here, `ess` is a vector so that each batch element can be treated independently.
            condition = ess < (
                self.config.system.resampling.ess_threshold * self.config.system.num_particles
            )

            step_metrics["resample"] = condition

            # Compute resampled particles for all batch elements.
            resampled_particles = self.resample(updated_particles, key_resampling, resample_logits)

            # Element-wise, if condition[i] is True, select the resampled particle; otherwise,
            # keep the original.
            def select_fn(new_field: chex.Array, old_field: chex.Array) -> chex.Array:
                # Broadcast condition to match the shape of each field.
                cond = condition.reshape((condition.shape[0],) + (1,) * (old_field.ndim - 1))
                return jnp.where(cond, new_field, old_field)

            updated_particles = jax.tree_util.tree_map(
                select_fn, resampled_particles, updated_particles
            )
            return (updated_particles, next_sampled_actions), step_metrics

        else:
            raise ValueError(f"Invalid resampling mode: {resampling_mode}")

    def init_particles(self, root: SPORootFnOutput) -> Particles:
        """
        Initialize particles at the start of the search.

        Args:
            root (SPORootFnOutput): The root object containing initial environment
                states and actions.

        Returns:
            Particles: Initialized particles with initial states, actions, and default metrics.
        """

        batch_size = root.particle_values.shape[0]
        particles = Particles(
            state_embedding=root.particle_env_states,
            root_actions=root.particle_actions,
            resample_td_weights=jnp.zeros(shape=(batch_size, self.config.system.num_particles)),
            prior_logits=root.particle_logits,
            value=root.particle_values,
            terminal=jnp.zeros(
                shape=(
                    batch_size,
                    self.config.system.num_particles,
                ),
                dtype=jnp.bool,
            ),
            depth=jnp.zeros(
                shape=(
                    batch_size,
                    self.config.system.num_particles,
                ),
                dtype=jnp.int32,
            ),
            gae=jnp.zeros(shape=(batch_size, self.config.system.num_particles), dtype=jnp.float32),
        )
        # Check shape for sanity
        chex.assert_tree_shape_prefix(
            particles,
            (
                batch_size,
                self.config.system.num_particles,
            ),
        )

        return particles

    def smc_weight_update_fn(
        self,
        particles: Particles,
        recurrent_output: SPORecurrentFnOutput,
    ) -> chex.Array:
        """
        Update temporal difference (TD) weights based on rewards and value estimates.

        Args:
            particles (Particles): Current particle states and metrics.
            recurrent_output (SPORecurrentFnOutput): Output from the recurrent function,
                including rewards and next values.

        Returns:
            chex.Array: Updated TD weights for each particle.
        """
        # Compute the TD error: reward + next value - current value
        # We do not multiply by discount as we do it in the recurrent_fn.
        td_error = recurrent_output.reward + recurrent_output.value - particles.value

        # Apply a terminal mask to ignore updates for terminal states
        terminal_mask = 1 - particles.terminal.astype(jnp.int32)

        # Update TD weights by accumulating the TD error, considering the terminal mask
        # we do not want to add td errors after autoresetting or particles die.
        next_td_weights = td_error * terminal_mask + particles.resample_td_weights

        # Validate the shape of the updated TD weights
        chex.assert_shape(
            next_td_weights,
            (
                particles.value.shape[0],
                self.config.system.num_particles,
            ),
        )

        return next_td_weights

    def get_resample_logits(
        self,
        td_weights: chex.Array,
        log_temperature: chex.Array | None = None,
        temperature: float | None = None,
    ) -> chex.Array:
        """
        Compute resampling logits from temporal difference (TD) weights and temperature.

        Args:
            td_weights (chex.Array): Temporal difference weights for each particle.
            log_temperature (chex.Array): Logarithm of the temperature parameter for scaling.

        Returns:
            chex.Array: Logits used for categorical resampling of particles.
        """

        # Convert log temperature to temperature and ensure numerical stability
        if log_temperature is not None:
            temperature = jax.nn.softplus(log_temperature).squeeze() + _SPO_FLOAT_EPSILON

        # Scale TD weights by temperature to obtain logits
        return td_weights / temperature

    def resample(
        self,
        particles: Particles,
        key: chex.Array,
        resample_logits: chex.Array,
    ) -> Particles:
        """
        Resample particles based on computed logits to focus on promising trajectories.

        Args:
            particles (Particles): Current particle states and metrics.
            key (chex.Array): RNG key for stochastic resampling.
            resample_logits (chex.Array): Logits determining the probability of selecting
                each particle.

        Returns:
            Particles: Resampled particles with reset TD weights and preserved advantages.
        """

        # Split the RNG key for resampling operations
        key, key_resample = jax.random.split(key)

        # Generate separate keys for each batch dimension
        batch_dim_keys = jax.random.split(key_resample, resample_logits.shape[0])
        # Sample indices for resampling using categorical distribution based on logits
        particle_selection_idxs = jax.vmap(jax.random.categorical, in_axes=(0, 0, None, None))(
            batch_dim_keys,
            resample_logits,
            -1,
            (self.config.system.num_particles,),
        )

        def get_particles(
            selection_idxs: chex.Array,
            non_batch_particles: Particles,
        ) -> Particles:
            """
            Select particles based on sampled indices for a single batch.

            Args:
                selection_idxs (chex.Array): Indices of selected particles for resampling.
                non_batch_particles (Particles): Particles from a single batch.

            Returns:
                Particles: Resampled particles for the batch.
            """
            return jax.tree_util.tree_map(  # type: ignore
                lambda x: x[selection_idxs], non_batch_particles
            )

        # Apply resampling across all batches
        particles_resampled = jax.vmap(get_particles, in_axes=(0, 0))(
            particle_selection_idxs,
            particles,
        )

        batch_size = particles.value.shape[0]

        # Reset the TD weights after resampling
        particles_resampled: Particles = particles_resampled._replace(
            resample_td_weights=jnp.zeros(shape=(batch_size, self.config.system.num_particles)),
        )
        # Preserve the Generalized Advantage Estimation (GAE) before resampling.
        # For the temperature loss, to correctly target the KL, we need use the advantages
        # before resampling has occurred. This means that the GAE/Advantage used is only calculated
        # up to the resampling period however, this is still good enough for the adaptive
        # temperature loss as it is still a suitable approximation of the advantage of sampled
        # actions from the policy.
        return particles_resampled._replace(gae=particles.gae)

    def update_particles(
        self,
        embedding: chex.ArrayTree,
        updated_td_weights: chex.Array,
        root_action: chex.Array,
        recurrent_output: SPORecurrentFnOutput,
        particles: Particles,
    ) -> Particles:
        """
        Update particle states with new embeddings, weights, actions, and metrics.

        Args:
            embedding (chex.ArrayTree): New state embeddings from the environment after action
                execution.
            updated_td_weights (chex.Array): Updated temporal difference weights.
            root_action (chex.Array): Actions taken from the root node.
            recurrent_output (SPORecurrentFnOutput): Output from the recurrent function, including
                new values and rewards.
            particles (Particles): Current particle states and metrics.

        Returns:
            Particles: Updated particle states with new embeddings, actions, weights, and metrics.
        """

        return Particles(
            state_embedding=embedding,
            root_actions=root_action,
            resample_td_weights=updated_td_weights,
            prior_logits=recurrent_output.prior_logits,
            value=recurrent_output.value,
            terminal=jnp.logical_or(
                particles.terminal, jnp.where(recurrent_output.discount == 0, True, False)
            ),
            depth=particles.depth + 1,
            gae=self.calculate_gae(
                current_gae=particles.gae,
                value=particles.value,
                next_value=recurrent_output.value,
                reward=recurrent_output.reward,
                discount=recurrent_output.discount,
                depth=particles.depth,
                gamma=self.config.system.search_gamma,
                lambda_=self.config.system.search_gae_lambda,
            ),
        )

    def calculate_gae(
        self,
        current_gae: chex.Array,
        value: chex.Array,
        next_value: chex.Array,
        reward: chex.Array,
        discount: chex.Array,
        depth: chex.Array,
        gamma: float,
        lambda_: float,
    ) -> chex.Array:
        """
        Calculate the Generalized Advantage Estimation (GAE) for each particle.
        This is an iterative calculation going forward in time, not backwards
        as usually done. This is calculated so we can optimise the temperature loss.

        Args:
            current_gae (chex.Array): Current GAE estimates.
            value (chex.Array): Current value estimates.
            next_value (chex.Array): Next value estimates from the recurrent function.
            reward (chex.Array): Rewards received after taking actions.
            discount (chex.Array): Discount factors (typically gamma) applied to future rewards.
            depth (chex.Array): Current depth in the rollout.
            gamma (float): Discount factor for future rewards.
            lambda_ (float): GAE lambda parameter for bias-variance trade-off.

        Returns:
            chex.Array: Updated GAE estimates for each particle.
        """
        # Compute the TD error (delta)
        delta = reward + next_value - value

        # Update GAE using the TD error and decay factors
        updated_gae_estimate = delta * (gamma * lambda_ * discount) ** (depth) + current_gae

        return updated_gae_estimate

    def calculate_ess_and_entropy(
        self,
        td_weights: chex.Array,
        log_temperature: chex.Array | None = None,
        temperature: float | None = None,
    ) -> chex.Array:
        """
        Calculate the Effective Sample Size (ESS) for a given set of TD weights and temperature.
        The ESS is defined as 1 / sum(w_i^2), where w_i are normalized weights computed using
        softmax.

        Args:
            td_weights (chex.Array): Temporal difference weights for each particle.
            log_temperature (chex.Array): Optional log temperature for adaptive scaling.
            temperature (float): Optional fixed temperature if not using adaptive scaling.

        Returns:
            chex.Array: Effective Sample Size per batch element.
        """

        # Compute the scaled logits from the TD weights.
        logits = self.get_resample_logits(td_weights, log_temperature, temperature)

        # Normalize the logits to obtain probabilities.
        weights = jax.nn.softmax(logits, axis=-1)

        # ESS is the inverse of the sum of squared normalized weights.
        ess = 1.0 / jnp.sum(weights**2, axis=-1)

        # Compute the entropy of the weights.
        entropy = -jnp.sum(weights * jnp.log(weights + jnp.finfo(weights.dtype).tiny), axis=-1)

        return ess, entropy


def get_warmup_fn(
    env: Environment,
    params: SPOParams,
    apply_fns: Tuple[ActorApply, CriticApply, SPORootFnApply, SPOApply],
    buffer_add_fn: Callable,
    config: DictConfig,
) -> Callable:
    """Create a function for warming up the replay buffer before training.

    This function generates trajectories for initial buffer population by stepping
    through the environment using the current policy.

    Args:
        env: Environment to interact with.
        params: Network parameters including actor, critic, and dual parameters.
        apply_fns: Tuple of network apply functions (actor, critic, root_fn, search_apply_fn).
        buffer_add_fn: Function for adding transitions to the replay buffer.
        config: Configuration dictionary containing algorithm parameters.

    Returns:
        A function that steps through the environment and adds transitions to the replay buffer.
    """

    _, _, root_fn, search_apply_fn = apply_fns

    def warmup(
        env_states: WrapperState,
        timesteps: TimeStep,
        buffer_states: BufferState,
        keys: chex.PRNGKey,
    ) -> Tuple[WrapperState, TimeStep, BufferState, chex.PRNGKey]:
        def _env_step(
            carry: Tuple[WrapperState, TimeStep, chex.PRNGKey], _: Any
        ) -> Tuple[Tuple[WrapperState, TimeStep, chex.PRNGKey], SPOTransition]:
            """Execute a single environment step during policy rollout."""

            env_state, last_timestep, key = carry
            # SELECT ACTION
            key, root_key, policy_key = jax.random.split(key, num=3)
            root = root_fn(params, last_timestep.observation, env_state.unwrapped_state, root_key)
            search_output = search_apply_fn(params, policy_key, root)
            action = search_output.action
            search_policy = search_output.sampled_action_weights
            search_value = search_output.value
            sampled_actions = search_output.sampled_actions

            # STEP ENVIRONMENT
            env_state, timestep = env.step(env_state, action)

            # LOG EPISODE METRICS
            done = (timestep.discount == 0.0).reshape(-1)
            truncated_step = (timestep.last() & (timestep.discount != 0.0)).reshape(-1)
            info = timestep.extras["episode_metrics"]
            # Get the timestep's (potentially final) observation for bootstrapping.
            # This is done to support truncated episodes.
            bootstrap_obs = timestep.extras["next_obs"]

            transition = SPOTransition(
                done,
                truncated_step,
                action,
                sampled_actions,
                search_policy,
                timestep.reward,
                search_value,
                last_timestep.observation,
                bootstrap_obs,
                info,
                search_output.sampled_advantages,
            )

            return (env_state, timestep, key), transition

        # STEP ENVIRONMENT FOR ROLLOUT LENGTH
        (env_states, timesteps, keys), traj_batch = jax.lax.scan(
            _env_step, (env_states, timesteps, keys), None, config.system.warmup_steps
        )

        # Add the trajectory to the buffer.
        # Swap the batch and time axes.
        traj_batch = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), traj_batch)

        buffer_states = buffer_add_fn(buffer_states, traj_batch)

        return env_states, timesteps, keys, buffer_states

    batched_warmup_step: Callable = jax.vmap(
        warmup, in_axes=(0, 0, 0, 0), out_axes=(0, 0, 0, 0), axis_name="batch"
    )

    return batched_warmup_step


def get_learner_fn(
    env: Environment,
    apply_fns: Tuple[ActorApply, CriticApply, SPORootFnApply, SPOApply],
    update_fns: Tuple[optax.TransformUpdateFn, optax.TransformUpdateFn, optax.TransformUpdateFn],
    buffer_fns: Tuple[Callable, Callable],
    config: DictConfig,
) -> LearnerFn[OffPolicyLearnerState]:
    """Create the learner function for training the SPO agent.

    This function builds a learner for the Sequential Policy Optimization (SPO) algorithm.
    The learner function handles the entire training loop, including:
    1. Collecting experience by interacting with the environment
    2. Storing transitions in the replay buffer
    3. Sampling experience batches for training
    4. Computing actor, critic, and dual losses
    5. Updating network parameters

    Args:
        env: Environment to interact with during training.
        apply_fns: Tuple of network apply functions (actor, critic, root_fn, search_apply_fn).
        update_fns: Tuple of optimizer update functions for actor, critic, and dual networks.
        buffer_fns: Tuple of buffer functions (add_fn, sample_fn) for replay buffer operations.
        config: Configuration dictionary containing algorithm parameters.

    Returns:
        A learner function that takes a learner state and returns updated state and metrics.
    """

    # Unpack apply functions for actor, critic, SPO root, and SPO search.
    actor_apply_fn, critic_apply_fn, root_fn, search_apply_fn = apply_fns

    # Unpack update functions for actor, critic, and dual networks.
    actor_update_fn, critic_update_fn, dual_update_fn = update_fns

    # Unpack buffer functions for adding and sampling trajectories.
    buffer_add_fn, buffer_sample_fn = buffer_fns

    def _update_step(
        learner_state: OffPolicyLearnerState, _: Any
    ) -> Tuple[OffPolicyLearnerState, Tuple]:
        """Execute a single update step of the SPO training loop.

        This function performs a complete update cycle, including:
        1. Collecting experience by rolling out the current policy in the environment
        2. Adding new transitions to the replay buffer
        3. Sampling experiences from the buffer
        4. Computing and applying updates to network parameters

        Args:
            learner_state: Current state of the learner including network parameters,
                optimizer states, buffer state, and environment state.
            _: Dummy argument for compatibility with jax.lax.scan.

        Returns:
            A tuple containing:
                - Updated learner state after the update step.
                - Metrics from the environment interactions and optimization.
        """

        def _env_step(
            learner_state: OffPolicyLearnerState, _: Any
        ) -> Tuple[OffPolicyLearnerState, Tuple[SPOTransition, Dict[str, chex.Array]]]:
            """Execute a single environment step during policy rollout."""
            params, opt_states, buffer_state, key, env_state, last_timestep = learner_state

            # SELECT ACTION
            key, root_key, policy_key = jax.random.split(key, num=3)
            root = root_fn(params, last_timestep.observation, env_state.unwrapped_state, root_key)
            search_output = search_apply_fn(params, policy_key, root)
            action = search_output.action
            search_policy = search_output.sampled_action_weights
            search_value = search_output.value
            sampled_actions = search_output.sampled_actions

            # STEP ENVIRONMENT
            env_state, timestep = env.step(env_state, action)

            # LOG EPISODE METRICS
            done = (timestep.discount == 0.0).reshape(-1)
            truncated_step = (timestep.last() & (timestep.discount != 0.0)).reshape(-1)
            info = timestep.extras["episode_metrics"]
            # Get the timestep's (potentially final) observation for bootstrapping.
            # This is done to support truncated episodes.
            bootstrap_obs = timestep.extras["next_obs"]

            transition = SPOTransition(
                done,
                truncated_step,
                action,
                sampled_actions,
                search_policy,
                timestep.reward,
                search_value,
                last_timestep.observation,
                bootstrap_obs,
                info,
                search_output.sampled_advantages,
            )
            learner_state = OffPolicyLearnerState(
                params, opt_states, buffer_state, key, env_state, timestep
            )
            return learner_state, (transition, search_output.rollout_metrics)

        # STEP ENVIRONMENT FOR ROLLOUT LENGTH
        learner_state, (traj_batch, search_metrics) = jax.lax.scan(
            _env_step, learner_state, None, config.system.rollout_length
        )
        params, opt_states, buffer_state, key, env_state, last_timestep = learner_state

        # Add the trajectory to the buffer.
        # Swap the batch and time axes.
        traj_batch = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), traj_batch)
        buffer_state = buffer_add_fn(buffer_state, traj_batch)

        def _update_epoch(update_state: Tuple, _: Any) -> Tuple:
            """Update the network for a single epoch."""

            def _actor_loss_fn(
                online_actor_params: FrozenDict,
                dual_params: CategoricalDualParams,
                target_actor_params: FrozenDict,
                sequence: SPOTransition,
            ) -> Tuple:
                """Calculate the actor loss."""

                # Merge leading dimensions for batch processing.
                sequence = jax.tree.map(lambda x: merge_leading_dims(x, 2), sequence)
                batch_size = sequence.reward.shape[0]

                # Prepare advantages for temperature loss computation.
                adv_for_temp_loss = sequence.sampled_advantages
                adv_for_temp_loss = jnp.swapaxes(adv_for_temp_loss, 0, 1)
                chex.assert_shape(adv_for_temp_loss, (config.system.num_particles, batch_size))

                # Prepare sampled actions.
                sampled_actions = sequence.sampled_actions
                sampled_actions = jnp.swapaxes(sampled_actions, 0, 1)
                chex.assert_shape(
                    sampled_actions,
                    (
                        config.system.num_particles,
                        batch_size,
                    ),
                )

                # Prepare normalized SMC weights.
                norm_smc_weights = sequence.sampled_actions_weights
                norm_smc_weights = jnp.swapaxes(norm_smc_weights, 0, 1)
                chex.assert_shape(
                    norm_smc_weights,
                    (config.system.num_particles, batch_size),
                )

                # Compute action distributions for online and lagging target parameters.
                online_action_distribution = actor_apply_fn(online_actor_params, sequence.obs)
                target_action_distribution = actor_apply_fn(target_actor_params, sequence.obs)

                # Ensure the distributions are of expected types.
                check_distribution_type(online_action_distribution)
                check_distribution_type(target_action_distribution)

                # Compute temperature and scaling parameters with numerical stability.
                temperature = (
                    jax.nn.softplus(dual_params.log_temperature).squeeze() + _SPO_FLOAT_EPSILON
                )
                alpha = jax.nn.softplus(dual_params.log_alpha).squeeze() + _SPO_FLOAT_EPSILON

                # Define batch dimensions.
                batch_size = online_action_distribution.logits.shape[0]

                # Compute normalized policy advantages weights (used for metrics)
                # and temperature loss.
                (
                    normalized_policy_adv_weights,
                    loss_temperature,
                ) = compute_weights_and_temperature_loss(
                    adv_for_temp_loss, config.system.epsilon, temperature
                )

                # Compute non-parametric KL divergence from normalized advantages weights.
                # This is used to check if we are achieving the targeted KL.
                kl_nonparametric = compute_nonparametric_kl_from_normalized_weights(
                    normalized_policy_adv_weights
                )

                # Compute cross-entropy losses for policy
                loss_policy = compute_cross_entropy_loss(
                    sampled_actions, norm_smc_weights, online_action_distribution
                )

                # Compute overall KL divergence without per-dimension constraints.
                kl_div = target_action_distribution.kl_divergence(online_action_distribution)
                chex.assert_shape(kl_div, (batch_size,))

                # Compute KL penalties and dual losses for between online and target policy.
                loss_kl_penalty, loss_alpha = compute_parametric_kl_penalty_and_dual_loss(
                    kl_div, alpha, config.system.epsilon_policy
                )

                # Aggregate dual losses.
                loss_dual = loss_alpha + loss_temperature

                # Total loss combines policy, dual, and KL penalties.
                loss = loss_policy + loss_dual + loss_kl_penalty

                loss_info = {
                    "loss_temperature": loss_temperature,
                    "loss_alpha": loss_alpha,
                    "loss_policy": loss_policy,
                    "loss_kl_penalty": loss_kl_penalty,
                    "loss_dual": loss_dual,
                    "adaptive_temperature": temperature,
                    "alpha": alpha,
                    "kl_nonparametric": kl_nonparametric,
                    "kl_nonparametric_relative": kl_nonparametric / config.system.epsilon,
                    "kl_div": kl_div,
                }

                return loss, loss_info

            def _critic_loss_fn(
                online_critic_params: FrozenDict,
                target_critic_params: FrozenDict,
                sequence: SPOTransition,
            ) -> Tuple:
                """Calculation of the critic loss."""

                # Predict current and target values using respective critic networks.
                pred_values = critic_apply_fn(online_critic_params, sequence.obs)

                target_v_tm1 = critic_apply_fn(target_critic_params, sequence.obs)
                target_v_t = critic_apply_fn(target_critic_params, sequence.bootstrap_obs)

                # Compute targets using Generalized Advantage Estimation (GAE).
                _, targets = batch_truncated_generalized_advantage_estimation(
                    sequence.reward,
                    (1 - sequence.done) * config.system.gamma,
                    config.system.gae_lambda,
                    v_tm1=target_v_tm1,
                    v_t=target_v_t,
                    truncation_t=sequence.truncated,
                )

                # Calculate L2 loss between predicted values and targets.
                value_loss = rlax.l2_loss(pred_values, targets).mean()

                # Scale the value loss by a coefficient.
                critic_total_loss = config.system.vf_coef * value_loss

                loss_info = {
                    "value_loss": value_loss,
                    "value_pred_std": pred_values.std(),
                    "value_pred_mean": pred_values.mean(),
                }

                return critic_total_loss, loss_info

            params, opt_states, buffer_state, key = update_state

            key, sample_key = jax.random.split(key, num=2)

            # SAMPLE SEQUENCES
            sequence_sample = buffer_sample_fn(buffer_state, sample_key)
            sequence: SPOTransition = sequence_sample.experience

            # CALCULATE ACTOR LOSS
            actor_grad_fn = jax.grad(_actor_loss_fn, argnums=(0, 1), has_aux=True)
            actor_dual_grads, actor_dual_loss_info = actor_grad_fn(
                params.actor_params.online,
                params.dual_params,
                params.actor_params.target,
                sequence,
            )

            # CALCULATE CRITIC LOSS
            critic_grad_fn = jax.grad(_critic_loss_fn, has_aux=True)
            critic_grads, critic_loss_info = critic_grad_fn(
                params.critic_params.online, params.critic_params.target, sequence
            )

            # Compute the parallel mean (pmean) over the batch.
            # This calculation is inspired by the Anakin architecture demo notebook.
            # available at https://tinyurl.com/26tdzs5x
            # This pmean could be a regular mean as the batch axis is on the same device.
            actor_dual_grads, actor_dual_loss_info = jax.lax.pmean(
                (actor_dual_grads, actor_dual_loss_info), axis_name="batch"
            )
            # pmean over devices.
            actor_dual_grads, actor_dual_loss_info = jax.lax.pmean(
                (actor_dual_grads, actor_dual_loss_info), axis_name="device"
            )
            # Separate the gradients
            actor_grads, dual_grads = actor_dual_grads

            critic_grads, critic_loss_info = jax.lax.pmean(
                (critic_grads, critic_loss_info), axis_name="batch"
            )
            # pmean over devices.
            critic_grads, critic_loss_info = jax.lax.pmean(
                (critic_grads, critic_loss_info), axis_name="device"
            )

            # UPDATE OPTIMISER STATE AND ACTOR
            actor_updates, actor_new_opt_state = actor_update_fn(
                actor_grads, opt_states.actor_opt_state, params.actor_params.online
            )
            new_online_actor_params = optax.apply_updates(params.actor_params.online, actor_updates)

            # UPDATE OPTIMISER STATE AND CRITIC
            critic_updates, critic_new_opt_state = critic_update_fn(
                critic_grads,
                opt_states.critic_opt_state,
                params.critic_params.online,
            )
            new_online_critic_params = optax.apply_updates(
                params.critic_params.online,
                critic_updates,
            )

            # Update dual network parameters using the optimizer.
            dual_updates, dual_new_opt_state = dual_update_fn(
                dual_grads,
                opt_states.dual_opt_state,
                params.dual_params,
            )
            # Apply updates to dual parameters and enforce constraints.
            dual_new_params = optax.apply_updates(params.dual_params, dual_updates)
            dual_new_params = clip_categorical_mpo_params(dual_new_params)

            # Incrementally update target parameters towards online parameters.
            new_target_actor_params = optax.incremental_update(
                new_online_actor_params, params.actor_params.target, config.system.tau
            )
            new_target_critic_params = optax.incremental_update(
                new_online_critic_params, params.critic_params.target, config.system.tau
            )

            # PACKING NEW PARAMS AND OPTIMISER STATE
            new_params = SPOParams(
                OnlineAndTarget(new_online_actor_params, new_target_actor_params),
                OnlineAndTarget(new_online_critic_params, new_target_critic_params),
                dual_new_params,
            )
            new_opt_state = SPOOptStates(
                actor_new_opt_state, critic_new_opt_state, dual_new_opt_state
            )

            # PACKING LOSS INFO
            loss_info = {
                **actor_dual_loss_info,
                **critic_loss_info,
            }
            return (new_params, new_opt_state, buffer_state, key), loss_info

        update_state = (params, opt_states, buffer_state, key)

        # UPDATE EPOCHS
        update_state, loss_info = jax.lax.scan(
            _update_epoch, update_state, None, config.system.epochs
        )

        params, opt_states, buffer_state, key = update_state
        learner_state = OffPolicyLearnerState(
            params, opt_states, buffer_state, key, env_state, last_timestep
        )
        metric = traj_batch.info
        loss_info.update(search_metrics)
        return learner_state, (metric, loss_info)

    def learner_fn(
        learner_state: OffPolicyLearnerState,
    ) -> AnakinExperimentOutput[OffPolicyLearnerState]:
        """Execute the SPO training loop for a series of update steps.

        This is the main learner function exposed to the training system. It handles
        batching and vectorization of update steps across devices, and wraps results
        in the expected experiment output format.

        Args:
            learner_state: Current state of the learner including network parameters,
                optimizer states, buffer state, and environment state.

        Returns:
            An AnakinExperimentOutput containing the updated learner state, episode metrics,
            and training metrics collected during the update steps.
        """

        batched_update_step = jax.vmap(_update_step, in_axes=(0, None), axis_name="batch")

        learner_state, (episode_info, loss_info) = jax.lax.scan(
            batched_update_step, learner_state, None, config.arch.num_updates_per_eval
        )
        return AnakinExperimentOutput(
            learner_state=learner_state,
            episode_metrics=episode_info,
            train_metrics=loss_info,
        )

    return learner_fn


def learner_setup(
    env: Environment, keys: chex.Array, config: DictConfig, model_env: Environment
) -> Tuple[LearnerFn[OffPolicyLearnerState], SPORootFnApply, SPOApply, OffPolicyLearnerState]:
    """Initialize all components needed for SPO training.

    This function handles the setup of networks, optimizers, environments, and initial states
    required for SPO training. It constructs the actor, critic, and dual networks,
    initializes parameters and optimizer states, creates the environment model for search,
    and prepares the replay buffer.

    Args:
        env: Training environment to interact with.
        keys: PRNG keys for initialization.
        config: Configuration dictionary containing algorithm parameters.
        model_env: Environment model used for SMC search.

    Returns:
        A tuple containing:
            - The learner function for training.
            - The root function for initializing SMC search.
            - The search apply function for executing SMC search.
            - Initial learner state containing parameters, optimizer states, and environment state.
    """
    # Get available TPU cores.
    n_devices = len(jax.devices())

    # Get number/dimension of actions.
    action_dim = int(env.action_space().num_values)
    config.system.action_dim = action_dim

    # PRNG keys.
    key, actor_net_key, critic_net_key = keys

    # Define network and optimiser.
    actor_torso = hydra.utils.instantiate(config.network.actor_network.pre_torso)
    actor_action_head = hydra.utils.instantiate(
        config.network.actor_network.action_head,
        action_dim=action_dim,
    )
    critic_torso = hydra.utils.instantiate(config.network.critic_network.pre_torso)
    critic_head = hydra.utils.instantiate(config.network.critic_network.critic_head)

    actor_network = Actor(torso=actor_torso, action_head=actor_action_head)
    critic_network = Critic(torso=critic_torso, critic_head=critic_head)

    actor_lr = make_learning_rate(
        config.system.actor_lr,
        config,
        config.system.epochs,
    )
    critic_lr = make_learning_rate(
        config.system.critic_lr,
        config,
        config.system.epochs,
    )

    actor_optim = optax.chain(
        optax.clip_by_global_norm(config.system.max_grad_norm),
        optax.adam(actor_lr, eps=1e-5),
    )
    critic_optim = optax.chain(
        optax.clip_by_global_norm(config.system.max_grad_norm),
        optax.adam(critic_lr, eps=1e-5),
    )

    # Initialise observation
    init_x = env.observation_space().generate_value()
    init_x = jax.tree_util.tree_map(lambda x: x[None, ...], init_x)

    # Initialise actor params and optimiser state.
    actor_params = actor_network.init(actor_net_key, init_x)
    actor_opt_state = actor_optim.init(actor_params)

    # Initialise critic params and optimiser state.
    critic_params = critic_network.init(critic_net_key, init_x)
    critic_opt_state = critic_optim.init(critic_params)

    # Initialise Dual params and optimiser state.
    log_temperature = jnp.full([1], config.system.init_log_temperature, dtype=jnp.float32)
    log_alpha = jnp.full([1], config.system.init_log_alpha, dtype=jnp.float32)

    dual_params = CategoricalDualParams(log_temperature=log_temperature, log_alpha=log_alpha)

    dual_lr = make_learning_rate(config.system.dual_lr, config, config.system.epochs)
    dual_optim = optax.chain(
        optax.clip_by_global_norm(config.system.max_grad_norm),
        optax.adam(dual_lr, eps=1e-5),
    )
    dual_opt_state = dual_optim.init(dual_params)

    # Pack params.
    params = SPOParams(
        OnlineAndTarget(actor_params, actor_params),
        OnlineAndTarget(critic_params, critic_params),
        dual_params,
    )

    actor_network_apply_fn = actor_network.apply
    critic_network_apply_fn = critic_network.apply

    root_fn = make_root_fn(actor_network_apply_fn, critic_network_apply_fn, config)
    environment_model_step = jax.vmap(model_env.step)
    model_recurrent_fn = make_recurrent_fn(
        environment_model_step, actor_network_apply_fn, critic_network_apply_fn, config
    )
    search_method = SPO(config, recurrent_fn=model_recurrent_fn)
    search_apply_fn = search_method.search

    # Pack apply and update functions.
    apply_fns = (
        actor_network_apply_fn,
        critic_network_apply_fn,
        root_fn,
        search_apply_fn,
    )
    update_fns = (actor_optim.update, critic_optim.update, dual_optim.update)

    dummy_info = {
        "episode_return": 0.0,
        "episode_length": 0,
        "is_terminal_step": False,
    }

    # Create replay buffer
    dummy_transition = SPOTransition(
        done=jnp.array(False),
        truncated=jnp.array(False),
        action=jnp.zeros((), dtype=jnp.int32),
        sampled_actions=jnp.zeros((config.system.num_particles,), dtype=jnp.int32),
        sampled_actions_weights=jnp.ones((config.system.num_particles,)),
        reward=jnp.array(0.0),
        search_value=jnp.array(0.0),
        obs=jax.tree_util.tree_map(lambda x: x.squeeze(0), init_x),
        bootstrap_obs=jax.tree_util.tree_map(lambda x: x.squeeze(0), init_x),
        info=dummy_info,
        sampled_advantages=jnp.zeros((config.system.num_particles,)),
    )

    assert config.system.total_buffer_size % n_devices == 0, (
        f"{Fore.RED}{Style.BRIGHT}The total buffer size should be divisible "
        + "by the number of devices!{Style.RESET_ALL}"
    )
    assert config.system.total_batch_size % n_devices == 0, (
        f"{Fore.RED}{Style.BRIGHT}The total batch size should be divisible "
        + "by the number of devices!{Style.RESET_ALL}"
    )
    config.system.buffer_size = config.system.total_buffer_size // (
        n_devices * config.arch.update_batch_size
    )
    config.system.batch_size = config.system.total_batch_size // (
        n_devices * config.arch.update_batch_size
    )
    buffer_fn = fbx.make_trajectory_buffer(
        max_size=config.system.buffer_size,
        min_length_time_axis=config.system.sample_sequence_length,
        sample_batch_size=config.system.batch_size,
        sample_sequence_length=config.system.sample_sequence_length,
        period=config.system.period,
        add_batch_size=config.arch.num_envs,
    )
    buffer_fns = (buffer_fn.add, buffer_fn.sample)
    buffer_states = buffer_fn.init(dummy_transition)

    # Get batched iterated update and replicate it to pmap it over cores.
    learn = get_learner_fn(env, apply_fns, update_fns, buffer_fns, config)
    learn = jax.pmap(learn, axis_name="device")

    warmup = get_warmup_fn(env, params, apply_fns, buffer_fn.add, config)
    warmup = jax.pmap(warmup, axis_name="device")

    # Initialise environment states and timesteps: across devices and batches.
    key, *env_keys = jax.random.split(
        key, n_devices * config.arch.update_batch_size * config.arch.num_envs + 1
    )
    env_states, timesteps = env.reset(jnp.stack(env_keys))
    reshape_states = lambda x: x.reshape(
        (n_devices, config.arch.update_batch_size, config.arch.num_envs) + x.shape[1:]
    )
    # (devices, update batch size, num_envs, ...)
    env_states = jax.tree_util.tree_map(reshape_states, env_states)
    timesteps = jax.tree_util.tree_map(reshape_states, timesteps)

    # Load model from checkpoint if specified.
    if config.logger.checkpointing.load_model:
        loaded_checkpoint = Checkpointer(
            model_name=config.system.system_name,
            **config.logger.checkpointing.load_args,  # Other checkpoint args
        )
        # Restore the learner state from the checkpoint
        restored_params, _ = loaded_checkpoint.restore_params(input_params=params)
        # Update the params
        params = restored_params

    # Define params to be replicated across devices and batches.
    key, step_key, warmup_key = jax.random.split(key, num=3)
    step_keys = jax.random.split(step_key, n_devices * config.arch.update_batch_size)
    warmup_keys = jax.random.split(warmup_key, n_devices * config.arch.update_batch_size)
    reshape_keys = lambda x: x.reshape((n_devices, config.arch.update_batch_size) + x.shape[1:])
    step_keys = reshape_keys(jnp.stack(step_keys))
    warmup_keys = reshape_keys(jnp.stack(warmup_keys))
    opt_states = SPOOptStates(actor_opt_state, critic_opt_state, dual_opt_state)
    replicate_learner = (params, opt_states, buffer_states)

    # Duplicate learner for update_batch_size.
    broadcast = lambda x: jnp.broadcast_to(x, (config.arch.update_batch_size,) + x.shape)
    replicate_learner = jax.tree_util.tree_map(broadcast, replicate_learner)

    # Duplicate learner across devices.
    replicate_learner = flax.jax_utils.replicate(replicate_learner, devices=jax.devices())

    # Initialise learner state.
    params, opt_states, buffer_states = replicate_learner
    # Warmup the buffer.
    env_states, timesteps, keys, buffer_states = warmup(
        env_states, timesteps, buffer_states, warmup_keys
    )
    init_learner_state = OffPolicyLearnerState(
        params, opt_states, buffer_states, step_keys, env_states, timesteps
    )

    return learn, root_fn, search_apply_fn, init_learner_state


def run_experiment(_config: DictConfig) -> float:
    """Runs experiment."""
    config = copy.deepcopy(_config)

    # Calculate total timesteps.
    n_devices = len(jax.devices())
    config.num_devices = n_devices
    config = check_total_timesteps(config)
    assert (
        config.arch.num_updates >= config.arch.num_evaluation
    ), "Number of updates per evaluation must be less than total number of updates."

    # Create the environments for train and eval.
    env, eval_env = environments.make(config=config)

    # PRNG keys.
    key, key_e, actor_net_key, critic_net_key = jax.random.split(
        jax.random.PRNGKey(config.arch.seed), num=4
    )

    # Setup learner.
    learn, root_fn, search_apply_fn, learner_state = learner_setup(
        env, (key, actor_net_key, critic_net_key), config, eval_env
    )

    # Setup evaluator.
    evaluator, absolute_metric_evaluator, (trained_params, eval_keys) = search_evaluator_setup(
        eval_env=eval_env,
        key_e=key_e,
        search_apply_fn=search_apply_fn,
        root_fn=root_fn,
        params=learner_state.params,
        config=config,
    )

    # Calculate number of updates per evaluation.
    config.arch.num_updates_per_eval = config.arch.num_updates // config.arch.num_evaluation
    steps_per_rollout = (
        n_devices
        * config.arch.num_updates_per_eval
        * config.system.rollout_length
        * config.arch.update_batch_size
        * config.arch.num_envs
    )

    # Logger setup
    logger = StoixLogger(config)
    logger.log_config(OmegaConf.to_container(config, resolve=True))
    print(f"{Fore.YELLOW}{Style.BRIGHT}JAX Global Devices {jax.devices()}{Style.RESET_ALL}")

    # Set up checkpointer
    save_checkpoint = config.logger.checkpointing.save_model
    if save_checkpoint:
        checkpointer = Checkpointer(
            metadata=config,  # Save all config as metadata in the checkpoint
            model_name=config.system.system_name,
            **config.logger.checkpointing.save_args,  # Checkpoint args
        )

    # Run experiment for a total number of evaluations.
    max_episode_return = -jnp.inf
    best_params = unreplicate_batch_dim(learner_state.params)
    for eval_step in range(config.arch.num_evaluation):
        # Train.
        start_time = time.time()

        learner_output = learn(learner_state)
        jax.block_until_ready(learner_output)

        # Log the results of the training.
        elapsed_time = time.time() - start_time
        t = int(steps_per_rollout * (eval_step + 1))
        episode_metrics, ep_completed = get_final_step_metrics(learner_output.episode_metrics)
        episode_metrics["steps_per_second"] = steps_per_rollout / elapsed_time

        # Separately log timesteps, actoring metrics and training metrics.
        logger.log({"timestep": t}, t, eval_step, LogEvent.MISC)
        if ep_completed:  # only log episode metrics if an episode was completed in the rollout.
            logger.log(episode_metrics, t, eval_step, LogEvent.ACT)
        train_metrics = learner_output.train_metrics
        # Calculate the number of optimiser steps per second. Since gradients are aggregated
        # across the device and batch axis, we don't consider updates per device/batch as part of
        # the SPS for the learner.
        opt_steps_per_eval = config.arch.num_updates_per_eval * (config.system.epochs)
        train_metrics["steps_per_second"] = opt_steps_per_eval / elapsed_time
        logger.log(train_metrics, t, eval_step, LogEvent.TRAIN)

        # Prepare for evaluation.
        start_time = time.time()
        trained_params = unreplicate_batch_dim(
            learner_output.learner_state.params
        )  # Select only actor params
        key_e, *eval_keys = jax.random.split(key_e, n_devices + 1)
        eval_keys = jnp.stack(eval_keys)
        eval_keys = eval_keys.reshape(n_devices, -1)

        # Evaluate.
        evaluator_output = evaluator(trained_params, eval_keys)
        jax.block_until_ready(evaluator_output)

        # Log the results of the evaluation.
        elapsed_time = time.time() - start_time
        episode_return = jnp.mean(evaluator_output.episode_metrics["episode_return"])

        steps_per_eval = int(jnp.sum(evaluator_output.episode_metrics["episode_length"]))
        evaluator_output.episode_metrics["steps_per_second"] = steps_per_eval / elapsed_time
        logger.log(evaluator_output.episode_metrics, t, eval_step, LogEvent.EVAL)

        if save_checkpoint:
            # Save checkpoint of learner state
            checkpointer.save(
                timestep=int(steps_per_rollout * (eval_step + 1)),
                unreplicated_learner_state=unreplicate_n_dims(learner_output.learner_state),
                episode_return=episode_return,
            )

        if config.arch.absolute_metric and max_episode_return <= episode_return:
            best_params = copy.deepcopy(trained_params)
            max_episode_return = episode_return

        # Update runner state to continue training.
        learner_state = learner_output.learner_state

    # Measure absolute metric.
    if config.arch.absolute_metric:
        start_time = time.time()

        key_e, *eval_keys = jax.random.split(key_e, n_devices + 1)
        eval_keys = jnp.stack(eval_keys)
        eval_keys = eval_keys.reshape(n_devices, -1)

        evaluator_output = absolute_metric_evaluator(best_params, eval_keys)
        jax.block_until_ready(evaluator_output)

        elapsed_time = time.time() - start_time
        t = int(steps_per_rollout * (eval_step + 1))
        steps_per_eval = int(jnp.sum(evaluator_output.episode_metrics["episode_length"]))
        evaluator_output.episode_metrics["steps_per_second"] = steps_per_eval / elapsed_time
        logger.log(evaluator_output.episode_metrics, t, eval_step, LogEvent.ABSOLUTE)

    # Stop the logger.
    logger.stop()
    # Record the performance for the final evaluation run. If the absolute metric is not
    # calculated, this will be the final evaluation run.
    eval_performance = float(jnp.mean(evaluator_output.episode_metrics[config.env.eval_metric]))
    return eval_performance


@hydra.main(
    config_path="../../configs/default/anakin",
    config_name="default_ff_spo.yaml",
    version_base="1.2",
)
def hydra_entry_point(cfg: DictConfig) -> float:
    """Experiment entry point."""
    # Allow dynamic attributes.
    OmegaConf.set_struct(cfg, False)

    # Run experiment.
    eval_performance = run_experiment(cfg)

    print(f"{Fore.CYAN}{Style.BRIGHT}SPO experiment completed{Style.RESET_ALL}")
    return eval_performance


if __name__ == "__main__":
    hydra_entry_point()
</file>

<file path="stoix/systems/search/evaluator.py">
from typing import Dict, Optional, Tuple

import chex
import jax
import jax.numpy as jnp
from flax.core.frozen_dict import FrozenDict
from omegaconf import DictConfig
from stoa.environment import Environment

from stoix.base_types import EvalFn, EvalState, EvaluationOutput
from stoix.systems.search.search_types import RootFnApply, SearchApply
from stoix.utils.jax_utils import unreplicate_batch_dim
from stoix.utils.running_statistics import RunningStatisticsState


def get_search_evaluator_fn(
    env: Environment,
    search_apply_fn: SearchApply,
    root_fn: RootFnApply,
    config: DictConfig,
    log_solve_rate: bool = False,
    eval_multiplier: int = 1,
) -> EvalFn:
    """Get the evaluator function for search-based algorithms.

    Args:
        env (Environment): An environment instance for evaluation.
        act_fn (callable): The act_fn that returns the action taken by the agent.
        config (dict): Experiment configuration.
        eval_multiplier (int): A scalar that will increase the number of evaluation
            episodes by a fixed factor. The reason for the increase is to enable the
            computation of the `absolute metric` which is a metric computed and the end
            of training by rolling out the policy which obtained the greatest evaluation
            performance during training for 10 times more episodes than were used at a
            single evaluation step.
    """

    def eval_one_episode(
        params: FrozenDict,
        init_eval_state: EvalState,
        running_statistics: Optional[RunningStatisticsState] = None,
    ) -> Dict:
        """Evaluate one episode. It is vectorized over the number of evaluation episodes."""

        def _env_step(eval_state: EvalState) -> EvalState:
            """Step the environment."""
            # PRNG keys.
            key, env_state, last_timestep, step_count, episode_return = eval_state

            # Select action.
            key, root_key, policy_key = jax.random.split(key, num=3)
            obs, model_env_state = jax.tree_util.tree_map(
                lambda x: x[jnp.newaxis, ...], (last_timestep.observation, env_state)
            )
            root = root_fn(params, obs, model_env_state, root_key)
            search_output = search_apply_fn(params, policy_key, root)
            action = search_output.action

            # Step environment.
            env_state, timestep = env.step(env_state, action.squeeze())

            # Log episode metrics.
            episode_return += timestep.reward
            step_count += 1
            eval_state = EvalState(key, env_state, timestep, step_count, episode_return)
            return eval_state

        def not_done(carry: Tuple) -> bool:
            """Check if the episode is done."""
            timestep = carry[2]
            is_not_done: bool = ~timestep.last()
            return is_not_done

        final_state = jax.lax.while_loop(not_done, _env_step, init_eval_state)

        eval_metrics = {
            "episode_return": final_state.episode_return,
            "episode_length": final_state.step_count,
        }
        # Log solved episode if solve rate is required.
        if log_solve_rate:
            eval_metrics["solved_episode"] = jnp.all(
                final_state.episode_return >= config.env.solved_return_threshold
            ).astype(int)

        return eval_metrics

    def evaluator_fn(
        trained_params: FrozenDict,
        key: chex.PRNGKey,
        running_statistics: Optional[RunningStatisticsState] = None,
    ) -> EvaluationOutput[EvalState]:
        """Evaluator function."""

        # Initialise environment states and timesteps.
        n_devices = len(jax.devices())

        eval_batch = (config.arch.num_eval_episodes // n_devices) * eval_multiplier

        key, *env_keys = jax.random.split(key, eval_batch + 1)
        env_states, timesteps = jax.vmap(env.reset)(
            jnp.stack(env_keys),
        )
        # Split keys for each core.
        key, *step_keys = jax.random.split(key, eval_batch + 1)
        # Add dimension to pmap over.
        step_keys = jnp.stack(step_keys).reshape(eval_batch, -1)

        eval_state = EvalState(
            key=step_keys,
            env_state=env_states,
            timestep=timesteps,
            step_count=jnp.zeros((eval_batch, 1)),
            episode_return=jnp.zeros_like(timesteps.reward),
        )

        eval_metrics = jax.vmap(
            eval_one_episode,
            in_axes=(None, 0, None),
            axis_name="eval_batch",
        )(trained_params, eval_state, running_statistics)

        return EvaluationOutput(
            learner_state=eval_state,
            episode_metrics=eval_metrics,
        )

    return evaluator_fn


def search_evaluator_setup(
    eval_env: Environment,
    key_e: chex.PRNGKey,
    search_apply_fn: SearchApply,
    root_fn: RootFnApply,
    params: FrozenDict,
    config: DictConfig,
) -> Tuple[EvalFn, EvalFn, Tuple[FrozenDict, chex.Array]]:
    """Initialise evaluator_fn."""
    # Get available TPU cores.
    n_devices = len(jax.devices())
    # Check if solve rate is required for evaluation.
    if hasattr(config.env, "solved_return_threshold"):
        log_solve_rate = True
    else:
        log_solve_rate = False

    evaluator = get_search_evaluator_fn(eval_env, search_apply_fn, root_fn, config, log_solve_rate)
    absolute_metric_evaluator = get_search_evaluator_fn(
        eval_env,
        search_apply_fn,
        root_fn,
        config,
        log_solve_rate,
        10,
    )

    evaluator = jax.pmap(evaluator, axis_name="device")
    absolute_metric_evaluator = jax.pmap(absolute_metric_evaluator, axis_name="device")

    # Broadcast trained params to cores and split keys for each core.
    trained_params = unreplicate_batch_dim(params)
    key_e, *eval_keys = jax.random.split(key_e, n_devices + 1)
    eval_keys = jnp.stack(eval_keys).reshape(n_devices, -1)

    return evaluator, absolute_metric_evaluator, (trained_params, eval_keys)
</file>

<file path="stoix/base_types.py">
from typing import Any, Callable, Dict, Generic, List, Optional, Tuple, TypeVar

import chex
from distrax import DistributionLike
from flashbax.buffers.trajectory_buffer import BufferState
from flax.core.frozen_dict import FrozenDict
from optax import OptState
from stoa import TimeStep, WrapperState
from typing_extensions import NamedTuple, Protocol, TypeAlias, runtime_checkable

from stoix.utils.running_statistics import RunningStatisticsState

Action: TypeAlias = chex.Array
Value: TypeAlias = chex.Array
Done: TypeAlias = chex.Array
Truncated: TypeAlias = chex.Array
First: TypeAlias = chex.Array
HiddenState: TypeAlias = chex.Array
LogProb: TypeAlias = chex.Array
Reward: TypeAlias = chex.Array
# Can't know the exact type of State.
State: TypeAlias = Any
Parameters: TypeAlias = Any
OptStates: TypeAlias = Any
HiddenStates: TypeAlias = Any
Metrics: TypeAlias = chex.ArrayTree


EvalResetFn = Callable[[chex.PRNGKey, int], Tuple[State, TimeStep]]


class Observation(NamedTuple):
    """The observation that the agent sees.
    agent_view: the agent's view of the environment.
    action_mask: boolean array specifying which action is legal.
    step_count: the number of steps elapsed since the beginning of the episode.
    """

    agent_view: chex.Array  # (num_obs_features,)
    action_mask: chex.Array  # (num_actions,)
    step_count: Optional[chex.Array] = None  # (,)


class ObservationGlobalState(NamedTuple):
    """The observation seen by agents in centralised systems.
    Extends `Observation` by adding a `global_state` attribute for centralised training.
    global_state: The global state of the environment, often a concatenation of agents' views.
    """

    agent_view: chex.Array
    action_mask: chex.Array
    global_state: chex.Array
    step_count: chex.Array


class EvalState(NamedTuple):
    """State of the evaluator."""

    key: chex.PRNGKey
    env_state: State
    timestep: TimeStep
    step_count: chex.Array
    episode_return: chex.Array


class RNNEvalState(NamedTuple):
    """State of the evaluator for recurrent architectures."""

    key: chex.PRNGKey
    env_state: State
    timestep: TimeStep
    dones: Done
    hstate: HiddenState
    step_count: chex.Array
    episode_return: chex.Array


class ActorCriticParams(NamedTuple):
    """Parameters of an actor critic network."""

    actor_params: FrozenDict
    critic_params: FrozenDict


class ActorCriticOptStates(NamedTuple):
    """OptStates of actor critic learner."""

    actor_opt_state: OptState
    critic_opt_state: OptState


class ActorCriticHiddenStates(NamedTuple):
    """Hidden states for an actor critic learner."""

    policy_hidden_state: HiddenState
    critic_hidden_state: HiddenState


class CoreLearnerState(NamedTuple):
    """Base state of the learner. Can be used for both on-policy and off-policy learners.
    Mainly used for sebulba systems since we dont store env state."""

    params: Parameters
    opt_states: OptStates
    key: chex.PRNGKey


class OnPolicyLearnerState(NamedTuple):
    """State of the learner. Used for on-policy learners."""

    params: Parameters
    opt_states: OptStates
    key: chex.PRNGKey
    env_state: WrapperState
    timestep: TimeStep


class RNNLearnerState(NamedTuple):
    """State of the `Learner` for recurrent architectures."""

    params: Parameters
    opt_states: OptStates
    key: chex.PRNGKey
    env_state: WrapperState
    timestep: TimeStep
    done: Done
    truncated: Truncated
    hstates: HiddenStates


class OffPolicyLearnerState(NamedTuple):
    params: Parameters
    opt_states: OptStates
    buffer_state: BufferState
    key: chex.PRNGKey
    env_state: WrapperState
    timestep: TimeStep


class RNNOffPolicyLearnerState(NamedTuple):
    params: Parameters
    opt_states: OptStates
    buffer_state: BufferState
    key: chex.PRNGKey
    env_state: WrapperState
    timestep: TimeStep
    dones: Done
    truncated: Truncated
    hstates: HiddenStates


class OnlineAndTarget(NamedTuple):
    online: FrozenDict
    target: FrozenDict


StoixState = TypeVar(
    "StoixState",
)
StoixTransition = TypeVar(
    "StoixTransition",
)


class SebulbaExperimentOutput(NamedTuple, Generic[StoixState]):
    """Experiment output."""

    learner_state: StoixState
    train_metrics: Dict[str, chex.Array]


class AnakinExperimentOutput(NamedTuple, Generic[StoixState]):
    """Experiment output."""

    learner_state: StoixState
    episode_metrics: Dict[str, chex.Array]
    train_metrics: Dict[str, chex.Array]


class EvaluationOutput(NamedTuple, Generic[StoixState]):
    """Evaluation output."""

    learner_state: StoixState
    episode_metrics: Dict[str, chex.Array]


RNNObservation: TypeAlias = Tuple[Observation, Done]
LearnerFn = Callable[[StoixState], AnakinExperimentOutput[StoixState]]
SebulbaLearnerFn = Callable[
    [StoixState, List[StoixTransition]], SebulbaExperimentOutput[StoixState]
]
SebulbaEvalFn = Callable[[FrozenDict, chex.PRNGKey], Dict[str, chex.Array]]

ActorApply = Callable[..., DistributionLike]

ActFn = Callable[[FrozenDict, Observation, chex.PRNGKey], chex.Array]
CriticApply = Callable[[FrozenDict, Observation], Value]
DistributionCriticApply = Callable[[FrozenDict, Observation], DistributionLike]
ContinuousQApply = Callable[[FrozenDict, Observation, Action], Value]
ActorCriticApply = Callable[[FrozenDict, Observation], Tuple[DistributionLike, Value]]
RecActorApply = Callable[
    [FrozenDict, HiddenState, RNNObservation], Tuple[HiddenState, DistributionLike]
]
RecActFn = Callable[
    [FrozenDict, HiddenState, RNNObservation, chex.PRNGKey], Tuple[HiddenState, chex.Array]
]
RecCriticApply = Callable[[FrozenDict, HiddenState, RNNObservation], Tuple[HiddenState, Value]]


@runtime_checkable
class EvalFn(Protocol[StoixState]):
    """Evaluator function protocol that allows for optional running_statistics parameter."""

    def __call__(
        self,
        trained_params: FrozenDict,
        key: chex.PRNGKey,
        running_statistics: Optional[RunningStatisticsState] = None,
    ) -> EvaluationOutput[StoixState]:
        ...
</file>

</files>
